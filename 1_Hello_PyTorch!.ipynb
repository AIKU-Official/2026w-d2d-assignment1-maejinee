{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtsHWi7KpR1x"
      },
      "source": [
        "# PyTorch 시작하기\n",
        "**AIKU 학회원 여러분!** 딥러닝을 향한 여정에 발 들인 것을 환영합니다😄.\n",
        "\n",
        "학회원 여러분이 앞으로 마주하게 될 여러 어려움들이 있을텐데, Deep Into Deep의 수업과 과제가 그 길에 조금은 도움이 되길 바랍니다.\n",
        "\n",
        "본 과제는 CS231n, 고려대학교 딥러닝 수업 등 여러 좋은 과제들을 혼합해 만들어 졌음을 미리 알립니다. 거인의 어깨를 만들어준 여러분 감사합니다!\n",
        "\n",
        "다시 한 번 만나서 반갑습니다! **Happy DeepLearning!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7e58EDipR1z"
      },
      "source": [
        "## 왜 Deep Learning frameworks를 써야할까요?\n",
        "\n",
        "* 우리의 Code를 GPU로 실행 시킬 수 있습니다! 이를 통해 모델을 훨씬 빠르게 훈련할 수 있습니다. PyTorch나 TensorFlow와 같은 프레임워크를 사용하면 CUDA 코드를 직접 작성할 필요 없이(이 강의의 범위를 벗어나는) 자신만의 맞춤형 신경망 아키텍처를 위해 GPU의 성능을 활용할 수 있습니다.\n",
        "\n",
        "* 이 강의에서는 프로젝트에 이러한 프레임워크 중 하나 (PyTorch)를 사용하여 사용하려는 모든 기능을 직접 작성할 때보다 더 효율적으로 실험할 수 있도록 준비할 것입니다.\n",
        "\n",
        "* 거인들의 어깨 위에 서 보시기 바랍니다! TensorFlow와 PyTorch는 모두 여러분의 삶을 훨씬 편하게 만들어줄 훌륭한 프레임워크이며, 이제 그 기능을 이해하셨으니 자유롭게 사용하셔도 됩니다 :)\n",
        "\n",
        "* 마지막으로, 학계나 업계에서 접할 수 있는 딥 러닝 코드에 노출되기를 바랍니다.\n",
        "\n",
        "## PyTorch란 무엇일까요?\n",
        "numpy의 ndarray와 비슷하게 동작하는 **Tensor objects**에 대해서 동적 computational graphs를 실행하는 시스템입니다. PyTorch는 사람이 직접 backpropagation을 계산할 필요가 없이 강력한 **자동 미분** 엔진을 제공합니다.\n",
        "\n",
        "## 어떻게 PyTorch를 배울 수 있을까요?\n",
        "이 과제만으로는 PyTorch 전반을 이해하는데 어려움이 있을 수 있습니다.\n",
        "\n",
        "여러 가지 방법이 추천되지만, 도움이 될 만한 사이트와 학습 방법을 알려드리겠습니다.\n",
        "\n",
        "PyTorch 공식 한국어 튜토리얼 :\n",
        "https://tutorials.pytorch.kr/beginner/basics/intro.html\n",
        "\n",
        "Stanford의 PyTorch 강의 : https://github.com/jcjohnson/pytorch-examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZQFg1RRpR11"
      },
      "source": [
        "# GPU\n",
        "`런타임 -> 런타임 유형 변경`을 클릭하고 `하드웨어 가속기` 아래에서 `GPU`를 선택하면 Colab에서 GPU 장치로 수동 전환할 수 있습니다. 런타임을 전환하면 커널이 다시 시작되므로 패키지를 가져오기 위해 다음 셀을 실행하기 전에 이 작업을 수행해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKvYIhDDpR12",
        "outputId": "a8018368-9d3b-4ed8-a099-9d544329684c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "\n",
        "USE_GPU = True\n",
        "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss.\n",
        "print_every = 100\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4q1RwwngAJz"
      },
      "source": [
        "`using device: cuda`가 나오면 성공입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swu-jxUigCVr"
      },
      "source": [
        "## What is 'CUDA'?\n",
        "\n",
        "\n",
        "> CUDA(\"Compute Unified Device Architecture\", 쿠다)는 그래픽 처리 장치(GPU)에서 수행하는 (병렬 처리) 알고리즘을 C 프로그래밍 언어를 비롯한 산업 표준 언어를 사용하여 작성할 수 있도록 하는 GPGPU 기술이다. -Wikipedia-\n",
        "\n",
        "GPU는 원래 그 이름에서도 알 수 있듯이 Graphic 연산을 위한 장치였습니다. 하지만 GPU가 병렬 처리를 매우 빠른 속도로 처리한다는 점에 주목하여, 일반적인 matrix 연산에 사용될 수 있는 GPGPU 기술이 제시되었습니다. NVIDA가 지원하는 CUDA를 통해 개발자들이 쉽게 GPU 상에서 병렬 처리 알고리즘을 실행할 수 있게 도와줍니다.\n",
        "\n",
        "지금 과제는 Colab에서 진행되므로 특별히 CUDA Version을 설정해 줄 필요가 없습니다. 그렇지만, 앞으로 Local, 또는 Server에서 Deep Learning 코드를 실행하다 보면 CUDA, PyTorch 버젼과 관련된 오류를 많이 마주할 것입니다. 그럴 땐 다음과 같은 기술을 검토해 보세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viZC941XecG6"
      },
      "source": [
        "# Google Mount\n",
        "\n",
        "* colab에 파일을 업로드 하는 경우 colab 런타임이 끊어져버리게 되면, 다시 파일을 업로드 해야 한다는 불상사가 생기게 됩니다.\n",
        "* colab에서는 google drive에 접근하여, google drive에 저장되어 있는 데이터셋에 접근할 수 있습니다.\n",
        "* 이번 과제에서는 사용하지 않지만, 앞으로의 과제를 위하여 미리 알아두는 것이 좋을 것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PuMfr_caenVB",
        "outputId": "05bfe66a-1980-47b2-dcb2-fc47a0d672dc"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fI9EE0pR12"
      },
      "source": [
        "# Part I. 준비\n",
        "이제 MNIST dataset을 활용한 Classification 과제를 시작해보겠습니다!\n",
        "\n",
        "준비단계에서는 학습을 위한 데이터셋을 불러오고, 전처리하는 단계까지 수행할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtmHmtOPfFpz"
      },
      "source": [
        "## MNIST Dataset이란?\n",
        "\n",
        "* MNIST는 간단한 컴퓨터 비전 데이터 세트로, 손으로 쓰여진 이미지들로 구성되어 있습니다.\n",
        "* 인공지능 연구의 권위자 LeCun교수가 만든 데이터 셋으로, MNIST Dataset을 활용한 분류 문제는 딥러닝 생태계의 Hello, World같은 역할을 합니다.\n",
        "\n",
        "* MNIST는 60,000개의 트레이닝 셋과 10,000개의 테스트 셋으로 이루어져 있고 이중 트레이닝 셋을 학습데이터로 사용하고 테스트 셋을 신경망을 검증하는 데에 사용합니다.\n",
        "\n",
        "* 숫자는 0에서 9까지의 값을 가지며, 이미지의 크기는(28x28 픽셀)로 고정되어 있습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srBzs8OIfuFT"
      },
      "source": [
        "이제 밑의 코드를 실행시켜 MNIST Dataset을 colab에 불러와주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tjMM3t2QfRDy",
        "outputId": "0ca7131c-5767-423f-e4fb-29d5f824f13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-23 05:08:02--  https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Resolving ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)... 3.5.25.206, 16.15.216.129, 3.5.29.47, ...\n",
            "Connecting to ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)|3.5.25.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9912422 (9.5M) [application/x-gzip]\n",
            "Saving to: ‘train-images-idx3-ubyte.gz’\n",
            "\n",
            "train-images-idx3-u 100%[===================>]   9.45M  4.28MB/s    in 2.2s    \n",
            "\n",
            "2026-01-23 05:08:05 (4.28 MB/s) - ‘train-images-idx3-ubyte.gz’ saved [9912422/9912422]\n",
            "\n",
            "--2026-01-23 05:08:05--  https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Resolving ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)... 3.5.21.122, 54.231.200.1, 16.15.186.12, ...\n",
            "Connecting to ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)|3.5.21.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28881 (28K) [application/x-gzip]\n",
            "Saving to: ‘train-labels-idx1-ubyte.gz’\n",
            "\n",
            "train-labels-idx1-u 100%[===================>]  28.20K   129KB/s    in 0.2s    \n",
            "\n",
            "2026-01-23 05:08:06 (129 KB/s) - ‘train-labels-idx1-ubyte.gz’ saved [28881/28881]\n",
            "\n",
            "--2026-01-23 05:08:06--  https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Resolving ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)... 3.5.21.122, 54.231.200.1, 16.15.186.12, ...\n",
            "Connecting to ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)|3.5.21.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1648877 (1.6M) [application/x-gzip]\n",
            "Saving to: ‘t10k-images-idx3-ubyte.gz’\n",
            "\n",
            "t10k-images-idx3-ub 100%[===================>]   1.57M  1.15MB/s    in 1.4s    \n",
            "\n",
            "2026-01-23 05:08:09 (1.15 MB/s) - ‘t10k-images-idx3-ubyte.gz’ saved [1648877/1648877]\n",
            "\n",
            "--2026-01-23 05:08:09--  https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Resolving ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)... 3.5.21.122, 54.231.200.1, 16.15.186.12, ...\n",
            "Connecting to ossci-datasets.s3.amazonaws.com (ossci-datasets.s3.amazonaws.com)|3.5.21.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4542 (4.4K) [application/x-gzip]\n",
            "Saving to: ‘t10k-labels-idx1-ubyte.gz’\n",
            "\n",
            "t10k-labels-idx1-ub 100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-01-23 05:08:10 (276 MB/s) - ‘t10k-labels-idx1-ubyte.gz’ saved [4542/4542]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# colab에서는 !{리눅스 명령어}를 활용하여, 리눅스 명령어를 처리할 수 있습니다.\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY87MX0tgHiJ"
      },
      "source": [
        "## Dataset / DataLoader\n",
        "\n",
        "딥러닝 모델을 학습시키기 위한 데이터를 불러올때 **Dataset**과 **DataLoader** 클래스를 자주 씁니다.\n",
        "\n",
        "**Dataset**은 데이터를 로드하고 전처리하는 데 사용되는 추상 클래스입니다. 데이터를 메모리에 한 번에 모두 로드하지 않고, 필요한 부분만을 로드하여 메모리 효율성을 높입니다.\n",
        "Dataset 클래스를 상속받은 CustomDataset을 만들고 싶다면, `__len__`과 `__getitem__` 메서드를 오버라이딩하여 데이터의 크기와 각 샘플에 접근할 수 있는 방법을 정의해주어야 합니다.\n",
        "Dataset class를 통해 데이터셋을 정의하면, 데이터를 불러올 때 필요한 전처리 작업을 포함하여 데이터에 쉽게 접근하고 조작할 수 있습니다.\n",
        "\n",
        "DataLoader는 Dataset 객체를 감싸고, 배치(batch) 단위로 데이터를 모델에 공급할 수 있도록 해줍니다.\n",
        "이는 대량의 데이터를 처리할 때 매우 유용하며, 훈련 속도를 크게 향상시킵니다.\n",
        "DataLoader는 데이터 샘플링, 셔플링, 병렬 처리를 지원하여 모델 훈련 과정에서 효율성을 극대화합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqUsPYBgXz_"
      },
      "source": [
        "### 문제 1\n",
        "\n",
        "코드의 빈칸을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDBJu2TJpR13"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, istrain, trans = None, path = '/content/'):\n",
        "    '''\n",
        "      inputs:\n",
        "        - istrain : train mode인지 확인\n",
        "        - trans : 데이터 전처리 및 데이터 증강을 위한 함수\n",
        "        - path : 데이터셋이 저장되어 있는 폴더 경로\n",
        "    '''\n",
        "    self.trans = trans\n",
        "\n",
        "    if istrain:\n",
        "      image_path = os.path.join(path, 'train-images-idx3-ubyte.gz')\n",
        "      label_path = os.path.join(path, 'train-labels-idx1-ubyte.gz')\n",
        "    else:\n",
        "      image_path = os.path.join(path, 't10k-images-idx3-ubyte.gz')\n",
        "      label_path = os.path.join(path, 't10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    with gzip.open(image_path, 'rb') as f:\n",
        "      # self.images는 모든 이미지를 담고 있는 변수입니다.\n",
        "      self.images = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "      self.images = self.images.reshape(-1, 1, 28, 28)\n",
        "\n",
        "    with gzip.open(label_path, 'rb') as f:\n",
        "      # self.labels는 모든 라벨을 담고 있는 변수입니다.\n",
        "      self.labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    ###########################################################################\n",
        "    # __len__(self) 함수는 데이터의 크기를 받아오는 함수입니다.               #\n",
        "    # self.images나 self.labels를 활용하여 전체 데이터의 크기를 반환해주세요. #\n",
        "    ###########################################################################\n",
        "    # FILL YOUR CODE HERE\n",
        "    return len(self.images)\n",
        "    ###########################################################################\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    ###########################################################################\n",
        "    # __getitem__(self, idx) 함수는 idx번째 데이터를 가져오는 함수입니다.     #\n",
        "    # self.images와 self.labels를 활용하여 idx번째 image와 label을 반환하세요.#\n",
        "    ###########################################################################\n",
        "    # FILL YOUR CODE HERE\n",
        "    image = self.images[idx]\n",
        "    label = self.labels[idx]\n",
        "    ###########################################################################\n",
        "    if self.trans:\n",
        "      image = self.trans(image)\n",
        "\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HvkrWW0Njkod"
      },
      "outputs": [],
      "source": [
        "# 이미지를 텐서로 바꾸고, 적당한 값으로 바꿔주기 위해 필요한 함수입니다.\n",
        "transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1HcZP0htjnqp"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(trans = transform, istrain = True)\n",
        "test_dataset = CustomDataset(trans = transform, istrain = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "932U6Ek4jrdD"
      },
      "source": [
        "데이터셋의 개수를 확인해보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3biIHQSjqqb",
        "outputId": "25790b93-3479-4b88-8abb-5eb9dafd96fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train dataset: 60000\n",
            "Number of test dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of train dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of test dataset: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkku-EHujvyc"
      },
      "source": [
        "### 문제 2\n",
        "\n",
        "* 딥러닝 데이터에서는 train, test dataset이외에도 validation dataset이라는 것이 있습니다. (강의 자료 참고)\n",
        "\n",
        "* validation dataset이 직접적으로 주어지지 않으면, 보통 train dataset에서 일부를 떼어와서 사용합니다.\n",
        "\n",
        "* pytorch에서 제공하는 `random_split`이라는 함수를 활용하여, 직접 train dataset과 validation dataset을 8:2 비율로 나눠주세요!\n",
        "\n",
        "* `random_split` Documentation : https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "En3XCG31jvA7"
      },
      "outputs": [],
      "source": [
        "train_size = int(len(train_dataset) * 0.8)\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPNLemoplxV6",
        "outputId": "885414ca-9f81-4e43-a182-22897f2ed7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train dataset: 48000\n",
            "Number of valid dataset: 12000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of train dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of valid dataset: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ZoDnoLlz6W"
      },
      "source": [
        "### 문제 3\n",
        "\n",
        "* `DataLoader`를 사용하여 데이터를 batch 단위로 불러올 수 있게 합니다.\n",
        "* 비어있는 부분을 채워주세요!\n",
        "* `DataLoader` Documentation : https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B70XE6ZPmTYZ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "train dataloader는 batch size가 64이며, 데이터셋이 랜덤으로 섞여 반환됩니다.\n",
        "val dataloader와 test dataloader는 batch size가 32이며, 데이터셋이 섞이지 않고 반환됩니다.\n",
        "'''\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCsAL5_VmyUI"
      },
      "source": [
        "### 문제 4, 5\n",
        "\n",
        "* 다음 코드들을 실행하고, 자신의 생각을 적는 란에 자유롭게 자신의 생각을 적어주세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLhJa12PnK11"
      },
      "source": [
        "---\n",
        "MNIST Dataset의 image를 label 별로 출력하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_3NTfzq8m9d8"
      },
      "outputs": [],
      "source": [
        "def get_samples_per_class(images, labels):\n",
        "    samples = {}\n",
        "    for img, lbl in zip(images, labels):\n",
        "        if lbl not in samples:\n",
        "            samples[lbl] = img\n",
        "        if len(samples) == 10:\n",
        "            break\n",
        "    return samples\n",
        "\n",
        "tmp_dataset = CustomDataset(istrain = True)\n",
        "samples = get_samples_per_class(tmp_dataset.images, tmp_dataset.labels)\n",
        "\n",
        "sorted_samples = sorted(samples.items())\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(7, 3))\n",
        "for i, (label, image) in enumerate(sorted_samples):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(image.squeeze(), cmap='gray')\n",
        "    ax.set_title(f'Label: {label}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmjXkJjSnY6g"
      },
      "source": [
        "MNIST Dataset의 label별 분포를 출력하는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uo1BpEnCnd2w"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(tmp_dataset.labels, return_counts=True)\n",
        "label_distribution = dict(zip(unique, counts))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(label_distribution.keys(), label_distribution.values(), color='blue')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Label Distribution in MNIST Training Set')\n",
        "plt.xticks(np.arange(10))\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxH646donied"
      },
      "source": [
        "**문제 4**\n",
        "\n",
        "다음 분포는 모델이 숫자를 인식하는 데에 있어 적합한 학습 데이터셋의 분포일까요?\n",
        "이유와 함께 말씀해주세요.\n",
        "\n",
        "* 답 : 적합하다.\n",
        "* 이유 : 딥러닝 모델의 학습 데이터셋이 적합하려면 데이터가 균형 잡힌 분포를 이루면서 충분히 많은 크기를 갖추어야 하는데, 해당 데이터셋은 이러한 조건을 모두 만족한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIwxejenvlA"
      },
      "source": [
        "MNIST Dataset의 label별 평균 이미지의 모습을 출력하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G3i3T7non10O"
      },
      "outputs": [],
      "source": [
        "mean_images = {}\n",
        "for label in range(10):\n",
        "    mean_images[label] = tmp_dataset.images[tmp_dataset.labels == label].mean(axis=0)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "for i, (label, mean_image) in enumerate(mean_images.items()):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(mean_image.squeeze(), cmap='gray')\n",
        "    ax.set_title(f'Label: {label}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv78Zc51n6Up"
      },
      "source": [
        "Random으로 2000개의 image를 불러와서 image feature의 차원을 축소하여 2차원 평면에 나타낸 결과입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kE_UKjZPn5XU"
      },
      "outputs": [],
      "source": [
        "n_samples = 2000\n",
        "indices = np.random.choice(len(tmp_dataset.images), n_samples, replace=False)\n",
        "images_subset = tmp_dataset.images[indices].reshape(n_samples, -1)\n",
        "labels_subset = tmp_dataset.labels[indices]\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(images_subset)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for label in np.unique(labels_subset):\n",
        "    indices = labels_subset == label\n",
        "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=label, alpha=0.5)\n",
        "plt.legend()\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE projection of MNIST dataset')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h9Dmmq-oKd_"
      },
      "source": [
        "**문제 5**\n",
        "\n",
        "실행 결과를 통해 데이터셋에 대해 알 수 있는 점을 2가지 이상 적어주세요.\n",
        "\n",
        "1.  데이터셋의 라벨별로 이미지의 평균을 출력했을 때 어떤 숫자인지 뚜렷하게 보인다는 점에서 각 라벨에 해당하는 이미지들끼리 어느 정도 비슷한 모습을 보인다는 점을 알 수 있다.\n",
        "2.  데이터셋의 이미지들의 차원을 축소해서 2차원으로 분포를 나타내어도 라벨별로 어느 정도 군집화가 이루어져 있다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg86udhQpmRE"
      },
      "source": [
        "# Part II. BackPropagation 이해하기\n",
        "\n",
        "딥러닝에서는 모델을 학습시키기 위해 BackPropagation 이라는 것을 활용합니다. 딥러닝의 학습에 굉장히 중요한 역할을 하는 것인 만큼, 편미분, Chain Rule에 대한 간략한 설명과 일부 함수를 직접 구현해보는 기회를 가져보고자 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3YssC4pqNA0"
      },
      "source": [
        "## 편미분\n",
        "\n",
        "* 실제 딥러닝에서는 하나의 변수만을 사용하는 것이 아닌 2개 이상의 변수를 사용하기 때문에, 편미분을 사용합니다.\n",
        "* 편미분은 $d$ 대신 $\\partial$ 기호를 사용하며, 변수가 2개 이상임에 유의해주시기 바랍니다.\n",
        "* $x_0$와 $x_1$이 변수로 주어질 때, $\\frac{\\partial f(x_0, x_1)}{\\partial x_0}$는 $x_0$를 변수로 보고, 나머지 변수들은 모두 상수로 본 채 미분을 한다고 생각하시면 됩니다.\n",
        "* Example\n",
        "\n",
        "\\begin{align}\n",
        "  f(x_0, x_1) = x_0^2 + x_1^2 + x_0x_1 \\\\\n",
        "  \\\\\n",
        "  \\frac{\\partial f(x_0, x_1)}{\\partial x_0} = 2x_0 + x_1\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fqPUvqU-qy9q"
      },
      "outputs": [],
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  for idx in range(x.size):\n",
        "      tmp_val = x[idx]\n",
        "      x[idx] = tmp_val + h\n",
        "      fxh1 = f(x)\n",
        "\n",
        "      x[idx] = tmp_val - h\n",
        "      fxh2 = f(x)\n",
        "\n",
        "      grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "      x[idx] = tmp_val\n",
        "\n",
        "\n",
        "  return grad\n",
        "\n",
        "def function(x):\n",
        "  return x[0]**2 + x[1]**2 + x[0]*x[1]\n",
        "\n",
        "x = np.array([4., 5.])\n",
        "numerical_diff(function, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Na9WUj8rMO6"
      },
      "source": [
        "## Chain rule\n",
        "\n",
        "딥러닝에서 연쇄 법칙(Chain Rule)은 역전파(backpropagation)를 수행하는 데 매우 중요한 역할을 합니다. 역전파 알고리즘은 신경망을 학습시키기 위해 사용되며, 출력 층에서 입력 층으로 오차를 전파하면서 각 층의 기울기를 계산합니다. 합성 함수 $f(g(x))$에 대해 연쇄 법칙은 다음과 같이 표현됩니다.\n",
        "\n",
        "\\begin{align}\n",
        "  \\frac{d}{dx}f(g(x))\n",
        "  = \\frac{dg(x)}{dx} \\cdot \\frac{df(g(x))}{dg(x)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "이는 손실 함수의 입력에 대한 기울기를 각 층의 기울기를 곱하여 계산할 수 있음을 의미합니다. 다음 코드는 연쇄 법칙을 적용한 경우와 적용하지 않은 경우의 미분을 비교하는 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yq3P-0INrglG"
      },
      "outputs": [],
      "source": [
        "def g(x):\n",
        "    return x**2\n",
        "\n",
        "def f(u):\n",
        "    return np.sin(u)\n",
        "\n",
        "def composite_function(x):\n",
        "    return f(g(x))\n",
        "\n",
        "def chain_rule_derivative(x):\n",
        "    u = g(x)\n",
        "    df_du = numerical_diff(f, u)\n",
        "    du_dx = numerical_diff(g, x)\n",
        "    return du_dx * df_du\n",
        "\n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4\n",
        "    return (f(x + h) - f(x - h)) / (2 * h)\n",
        "\n",
        "x = np.array([2.0])\n",
        "chain_rule_result = chain_rule_derivative(x)\n",
        "numerical_result = numerical_diff(composite_function, x)\n",
        "\n",
        "print(f\"result of chain rule derivative : {chain_rule_result}\")\n",
        "print(f\"result of numerical differentiation : {numerical_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KW9hGALuXCZ"
      },
      "source": [
        "## 밑바닥부터 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLO6iHpBpR15"
      },
      "source": [
        "### Flatten (Using numpy)\n",
        "\n",
        "\n",
        "PyTorch Tensors는 개념적으로 n차원 배열과 유사합니다. n차원 숫자 그리드이며, PyTorch는 n차원 배열과 마찬가지로 텐서에서 효율적으로 작동할 수 있는 많은 함수를 제공합니다. 간단한 예로, 아래에서는 완전히 연결된 신경망에서 사용할 수 있도록 이미지 데이터를 재구성하는 `flatten` 함수를 제공합니다.\n",
        "\n",
        "이미지 데이터는 일반적으로 N x C x H x W 형태의 텐서에 저장된다는 점을 기억하세요:\n",
        "\n",
        "* N은 데이터 포인트의 수입니다.\n",
        "* C는 채널 수입니다.\n",
        "* H는 중간 특징 맵의 픽셀 단위 높이입니다.\n",
        "* W는 중간 피처 맵의 높이(픽셀)입니다.\n",
        "\n",
        "이는 2D convolution 같이 중간 특징이 서로 상대적인 위치에 대한 공간적 이해가 필요한 작업을 수행할 때 데이터를 표현하는 데 적합한 방법입니다. 그러나 fully connected affine layers를 사용하여 이미지를 처리할 때는 각 데이터 포인트를 단일 벡터로 표현해야 하므로 데이터의 여러 채널, 행, 열을 분리하는 것은 더 이상 유용하지 않습니다. 따라서 'flatten' 연산을 사용하여 표현당 `C x H x W` 값을 하나의 긴 벡터로 축소합니다. 아래의 flatten 함수는 먼저 주어진 데이터 배치에서 N, C, H, W 값을 읽은 다음 해당 데이터의 'view'를 반환합니다. \"view\"는 numpy의 \"reshape\" 메서드와 유사합니다. x의 차원을 N x ?? 로 재형성하며, 여기서 ?? 는 무엇이든 허용됩니다(이 경우 C x H x W가 되지만 명시적으로 지정할 필요는 없습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUnA0EncpR15",
        "outputId": "a0eeddc0-3475-4401-b7e0-59c9ac01f7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before flattening:  tensor([[[[ 0,  1],\n",
            "          [ 2,  3],\n",
            "          [ 4,  5]]],\n",
            "\n",
            "\n",
            "        [[[ 6,  7],\n",
            "          [ 8,  9],\n",
            "          [10, 11]]]])\n",
            "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "def flatten(x):\n",
        "    N = x.shape[0] # 데이터의 개수(배치 사이즈)를 먼저 파악한다.\n",
        "    return x.view(N, -1) # N행은 유지하고, 나머지 차원(C,H,W)은 곱해서 하나로 합친다.\n",
        "\n",
        "def test_flatten():\n",
        "    x = torch.arange(12).view(2, 1, 3, 2)\n",
        "    print('Before flattening: ', x)\n",
        "    print('After flattening: ', flatten(x))\n",
        "\n",
        "test_flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEL8vbVfu7rJ"
      },
      "source": [
        "### Sigmoid\n",
        "\n",
        "Sigmoid는 Gradient Vanishing 문제 때문에 딥러닝에서 많이 활용되지 못하고 있지만, 단순한 task에서는 아직까지 쓰이고 있습니다. Sigmoid 함수의 forward 및 backward 식은 다음과 같습니다. (backward는 직접 미분하면서 증명해보세요!)\n",
        "\n",
        "**forward**\n",
        "\n",
        "\\begin{align}\n",
        "  y = \\sigma(x) = \\frac{1}{1 + \\mbox{exp}(-x)}\n",
        "\\end{align}\n",
        "\n",
        "**backward**\n",
        "\\begin{align}\n",
        "  \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = dout \\cdot \\sigma(x) \\cdot (1 - \\sigma(x))\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Ik9TqnnuvEPM"
      },
      "outputs": [],
      "source": [
        "class Sigmoid():\n",
        "  def __init__(self):\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = 1 / (1+np.exp(-x))\n",
        "    self.y = y\n",
        "    return y\n",
        "\n",
        "  def backward(self, dout):\n",
        "    return dout * (1-self.y) * self.y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsVplnGJuhDY"
      },
      "source": [
        "### 문제 6. ReLU\n",
        "\n",
        "ReLU는 딥러닝에서 가장 많이 사용하는 활성화 함수 중 하나입니다. ReLU의 forward와 backward 식은 다음과 같습니다.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**forward**\n",
        "\\begin{align}\n",
        "  y =\n",
        "  \\begin{cases}\n",
        "    0, & x \\leq 0\\\\\n",
        "    x, & x > 0\n",
        "  \\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "\\\\\n",
        "\n",
        "**backward**\n",
        "\\begin{align}\n",
        "  \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} =\n",
        "  \\begin{cases}\n",
        "    0, & x \\leq 0\\\\\n",
        "    dout, & x > 0\n",
        "  \\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "\\\\\n",
        "\n",
        "Sigmoid 예시를 보고, ReLU의 forward, backward 빈칸을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4IT6ovE0uzvu"
      },
      "outputs": [],
      "source": [
        "class ReLU():\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    # FILL YOUR CODE HERE\n",
        "    self.mask = (x <= 0)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    # FILL YOUR CODE HERE\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1mxlj_IvGf1"
      },
      "source": [
        "### 문제 7. Linear\n",
        "\n",
        "`Linear`는 PyTorch에서 자주 사용되는 레이어로, 입력 데이터를 선형 변환하는 역할을 합니다. 선형 변환은 다음과 같은 수식으로 표현됩니다:\n",
        "\n",
        "**forward**\n",
        "\\begin{align}\n",
        "\\mathbf{y} = \\mathbf{x}W + \\mathbf{b}\n",
        "\\end{align}\n",
        "\n",
        "**backward**\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W} = x^T \\cdot dout \\\\\n",
        "\\\\\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial b} = \\sum_{i=1}^{N} dout_i \\\\\n",
        "\\\\\n",
        "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = dout \\cdot W^T\n",
        "\\end{align}\n",
        "\n",
        "여기서 x는 입력 벡터, W는 가중치 행렬, b는 편향 벡터입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2uJtgN5QvLbC"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    self.W = np.random.randn(input_dim, output_dim) * 0.01\n",
        "    self.b = np.zeros(output_dim)\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    # FILL YOUR CODE HERE\n",
        "    self.x = x\n",
        "    out = np.dot(self.x, self.W) + self.b\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    # FILL YOUR CODE HERE\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHNc34Au-EI0"
      },
      "source": [
        "### 문제 8. CrossEntropyLoss\n",
        "\n",
        "교차 엔트로피 손실은 분류 문제에서 모델의 성능을 평가하는 데 사용되는 손실 함수입니다. 이 함수는 모델이 예측한 확률 분포와 실제 타겟 분포 간의 차이를 측정합니다.\n",
        "\n",
        "**Softmax Function**\n",
        "\n",
        "* 소프트맥스 함수는 모델의 출력 로짓 값을 확률 분포로 변환합니다. 각 입력 값의 지수 함수를 계산한 후, 이 값들의 합으로 나누어 확률로 변환합니다.\n",
        "\n",
        "\\begin{align}\n",
        "\t\\mbox{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} = \\frac{e^{(z_i - c)}}{\\sum_{j} e^{(z_j - c)}}\n",
        "\\end{align}\n",
        "\n",
        "* 여기서 $z_i$는 입력 벡터의 i번째 원소입니다. 각 원소의 지수 값을 계산한 후 이 지수 값들의 합으로 나누어 각 원소를 확률로 변환합니다.\n",
        "\n",
        "**Cross Entropy Calculation - forward**\n",
        "* 교차 엔트로피 손실은 예측된 확률 분포와 실제 타겟 분포 간의 차이를 계산합니다. 실제 타겟 값의 원-핫 인코딩 벡터와 소프트맥스 출력 값을 사용하여 손실을 계산합니다.\n",
        "\n",
        "\\begin{align}\n",
        "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{ic} \\log(\\hat{y}_{ic})\n",
        "\\end{align}\n",
        "\n",
        "* 여기서:\n",
        "  * $N$은 배치 크기\n",
        "  * $C$는 클래스의 수\n",
        "  * $y_{ic}$는 i번째 샘플의 실제 클래스의 원-핫 인코딩 값\n",
        "  * $\\hat{y}_{ic}$는 i번째 샘플의 소프트맥스 출력 값\n",
        "\n",
        "**backward**\n",
        "* 역전파를 통해 손실 함수에 대한 그래디언트를 계산합니다. 각 로짓 값에 대한 그래디언트는 다음과 같은 수식으로 계산됩니다.\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{i}} = \\frac{1}{N} (\\hat{y}_{i} - y_{i})\n",
        "\\end{align}\n",
        "\n",
        "* 여기서:\n",
        "  * $\\hat{y}_{i}$는 소프트맥스 출력 값\n",
        "  * $y_{i}$는 실제 타겟 값\n",
        "\n",
        "* 이 수식을 통해 각 로짓 값에 대한 그래디언트를 계산하여 역전파를 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "boYn8sFt-C1a"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.softmax_value = None\n",
        "        self.pred = None\n",
        "        self.target = None\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def to_one_hot(self, y, batch_size, num_classes):\n",
        "        one_hot = np.zeros((batch_size, num_classes))\n",
        "        one_hot[np.arange(batch_size), y] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        CrossEntropyLoss의 forward 패스.\n",
        "\n",
        "        Inputs:\n",
        "          - pred (numpy.ndarray): 예측한 로짓 값.\n",
        "          - target (numpy.ndarray): 실제 타겟 값 (원-핫 인코딩 형식).\n",
        "\n",
        "        Returns:\n",
        "          - loss: 배치에 대한 평균 교차 엔트로피 손실.\n",
        "        \"\"\"\n",
        "        batch_size, num_classes = pred.shape\n",
        "\n",
        "        self.pred = pred\n",
        "        self.target = self.to_one_hot(target, batch_size, num_classes)\n",
        "\n",
        "        # FILL YOUR CODE HERE\n",
        "        self.softmax_value = self.softmax(pred)\n",
        "        loss = -np.sum(self.target * np.log(self.softmax_value + 1e-7)) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        CrossEntropyLoss의 backward 패스.\n",
        "\n",
        "        Inputs:\n",
        "          - dout : 1\n",
        "\n",
        "        Returns:\n",
        "          - grad: 예측값에 대한 그래디언트.\n",
        "        \"\"\"\n",
        "        # FILL YOUR CODE HERE\n",
        "        batch_size = self.target.shape[0]\n",
        "        grad = (self.softmax_value - self.target) / batch_size\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgkUWAzazIb3"
      },
      "source": [
        "### SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8esChpeqzFEj"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "  def __init__(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def step(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.learning_rate * grads[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISnEux55zXgQ"
      },
      "source": [
        "### 문제 9. 전체 학습 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dIVcuPCEzkML"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class ThreeLayerNet:\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "    self.params = {}\n",
        "    self.layers = OrderedDict()\n",
        "\n",
        "    self.layers['fc1'] = Linear(input_dim, hidden_dims[0])\n",
        "    self.layers['relu1'] = ReLU()\n",
        "    self.layers['fc2'] = Linear(hidden_dims[0], hidden_dims[1])\n",
        "    self.layers['relu2'] = ReLU()\n",
        "    self.layers['fc3'] = Linear(hidden_dims[1], output_dim)\n",
        "\n",
        "    self.params['W1'] = self.layers['fc1'].W\n",
        "    self.params['b1'] = self.layers['fc1'].b\n",
        "    self.params['W2'] = self.layers['fc2'].W\n",
        "    self.params['b2'] = self.layers['fc2'].b\n",
        "    self.params['W3'] = self.layers['fc3'].W\n",
        "    self.params['b3'] = self.layers['fc3'].b\n",
        "\n",
        "    self.last_layer = CrossEntropyLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    flatten을 사용하여 tensor를 2차원으로 만들고, self.layers에 있는 layer들을 사용하여, output을 만들어주세요!\n",
        "    '''\n",
        "    # FILL YOUR CODE HERE\n",
        "    x = flatten(x)\n",
        "    layers = list(self.layers.values())\n",
        "    for layer in layers:\n",
        "      x = layer.forward(x)\n",
        "    out = x\n",
        "    return out\n",
        "\n",
        "  def loss(self, y, t):\n",
        "    return self.last_layer.forward(y, t)\n",
        "\n",
        "  def accuracy(self, y, t):\n",
        "    y = np.argmax(y, axis = 1)\n",
        "    accuracy = np.sum(y == t) / float(t.shape[0])\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def gradient(self):\n",
        "\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "\n",
        "\n",
        "    grads['W1'] = self.layers['fc1'].dW\n",
        "\n",
        "    '''\n",
        "    위의 예시처럼 grads에 각각의 parameter의 gradient값을 dictionary 형태로 저장해주세요.\n",
        "    '''\n",
        "    # FILL YOUR CODE HERE\n",
        "    grads['b1'] = self.layers['fc1'].db\n",
        "    grads['W2'] = self.layers['fc2'].dW\n",
        "    grads['b2'] = self.layers['fc2'].db\n",
        "    grads['W3'] = self.layers['fc3'].dW\n",
        "    grads['b3'] = self.layers['fc3'].db\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "l8OmtcKf6qbN"
      },
      "outputs": [],
      "source": [
        "optimizer = SGD(learning_rate = 0.1)\n",
        "model = ThreeLayerNet(input_dim = 784, hidden_dims = [256, 64], output_dim=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_x3oDuva7Jgi",
        "outputId": "56a5a741-6a16-4f63-a5bf-c174b35e3b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/750 [00:00<?, ?it/s]/tmp/ipython-input-3130112219.py:12: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
            "  out = np.dot(self.x, self.W) + self.b\n",
            "/tmp/ipython-input-3130112219.py:17: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
            "  self.dW = np.dot(self.x.T, dout)\n",
            "  2%|▏         | 18/750 [00:00<00:08, 85.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 2.3021 Train Accuracy: 0.1562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 116/750 [00:01<00:06, 91.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 2.2144 Train Accuracy: 0.2188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 205/750 [00:02<00:06, 89.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 1.1261 Train Accuracy: 0.6562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 317/750 [00:05<00:07, 59.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 0.4656 Train Accuracy: 0.7969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 417/750 [00:06<00:03, 86.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 0.4323 Train Accuracy: 0.9062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 516/750 [00:07<00:02, 85.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 0.3330 Train Accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 614/750 [00:08<00:01, 89.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 0.2654 Train Accuracy: 0.9062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 714/750 [00:09<00:00, 89.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Train Loss: 0.4889 Train Accuracy: 0.8594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 73.92it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 178.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 1 Valid Accuracy: 0.8438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 9/750 [00:00<00:08, 87.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.3858 Train Accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 113/750 [00:01<00:07, 90.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.1629 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 210/750 [00:02<00:06, 88.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.5852 Train Accuracy: 0.8438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 315/750 [00:03<00:05, 84.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.3016 Train Accuracy: 0.9219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 414/750 [00:04<00:03, 84.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.3130 Train Accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 512/750 [00:05<00:02, 88.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.3710 Train Accuracy: 0.9062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 612/750 [00:06<00:01, 88.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.1831 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 711/750 [00:07<00:00, 90.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Train Loss: 0.1979 Train Accuracy: 0.9219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:08<00:00, 89.09it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 179.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 2 Valid Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 10/750 [00:00<00:08, 92.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.0459 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 110/750 [00:01<00:06, 92.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.2542 Train Accuracy: 0.9219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 218/750 [00:02<00:05, 91.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.0947 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 318/750 [00:03<00:04, 91.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.1427 Train Accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 417/750 [00:04<00:03, 92.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.2482 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 501/750 [00:05<00:05, 47.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.2365 Train Accuracy: 0.9062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 613/750 [00:08<00:01, 80.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.0633 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 720/750 [00:09<00:00, 92.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Train Loss: 0.2695 Train Accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 74.07it/s]\n",
            "100%|██████████| 1500/1500 [00:06<00:00, 228.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 3 Valid Accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 10/750 [00:00<00:08, 90.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.1379 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 119/750 [00:01<00:06, 92.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.0892 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 208/750 [00:03<00:15, 35.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.0800 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 315/750 [00:05<00:04, 87.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.0548 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 414/750 [00:06<00:03, 90.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.2128 Train Accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▊   | 514/750 [00:07<00:02, 91.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.1332 Train Accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 614/750 [00:08<00:01, 91.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.0440 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 714/750 [00:09<00:00, 91.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Train Loss: 0.0587 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 74.32it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 181.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 4 Valid Accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 10/750 [00:00<00:08, 91.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.0547 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 110/750 [00:01<00:06, 94.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.2509 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 220/750 [00:02<00:05, 95.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.0672 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████▏     | 310/750 [00:03<00:04, 92.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.1185 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 410/750 [00:04<00:03, 92.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.0999 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 510/750 [00:05<00:02, 87.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.1641 Train Accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 616/750 [00:06<00:01, 86.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.1210 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 716/750 [00:07<00:00, 88.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Train Loss: 0.1435 Train Accuracy: 0.9531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:08<00:00, 90.57it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 176.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 5 Valid Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 9/750 [00:00<00:08, 84.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.0418 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 119/750 [00:01<00:06, 92.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.1715 Train Accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 219/750 [00:02<00:05, 93.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.0886 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 316/750 [00:03<00:04, 90.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.0265 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 416/750 [00:04<00:03, 91.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.1308 Train Accuracy: 0.9219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 503/750 [00:06<00:09, 25.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.0231 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 618/750 [00:08<00:01, 86.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.0616 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 717/750 [00:09<00:00, 92.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Train Loss: 0.0290 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 74.61it/s]\n",
            "100%|██████████| 1500/1500 [00:06<00:00, 228.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 6 Valid Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 9/750 [00:00<00:08, 85.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0358 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 106/750 [00:01<00:07, 85.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0361 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 209/750 [00:03<00:11, 46.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0423 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 317/750 [00:05<00:04, 87.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0639 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 416/750 [00:06<00:03, 89.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0118 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 516/750 [00:07<00:02, 89.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0180 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 616/750 [00:08<00:01, 89.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0663 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 712/750 [00:09<00:00, 87.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Train Loss: 0.0327 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 74.41it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 177.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 7 Valid Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 8/750 [00:00<00:09, 77.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0236 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 112/750 [00:01<00:07, 88.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0667 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 210/750 [00:02<00:06, 86.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0416 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 316/750 [00:03<00:05, 85.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0716 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 415/750 [00:04<00:03, 89.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0680 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▊   | 514/750 [00:05<00:02, 79.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0620 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 612/750 [00:06<00:01, 91.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0569 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 712/750 [00:08<00:00, 90.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Train Loss: 0.0690 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:08<00:00, 88.03it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 180.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 8 Valid Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 9/750 [00:00<00:08, 84.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0128 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 115/750 [00:01<00:06, 90.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0453 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▊       | 215/750 [00:02<00:06, 86.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0266 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████▏     | 310/750 [00:03<00:05, 83.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0264 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 408/750 [00:04<00:04, 84.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0127 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 505/750 [00:07<00:06, 40.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0180 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████▏ | 611/750 [00:08<00:01, 89.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0190 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 713/750 [00:09<00:00, 85.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Train Loss: 0.0221 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 73.07it/s]\n",
            "100%|██████████| 1500/1500 [00:06<00:00, 225.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 9 Valid Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 9/750 [00:00<00:08, 87.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0045 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▎        | 102/750 [00:01<00:19, 32.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0140 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▊       | 214/750 [00:04<00:06, 83.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0173 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 319/750 [00:05<00:04, 92.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0396 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 418/750 [00:06<00:03, 91.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0443 Train Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 508/750 [00:07<00:02, 84.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0417 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 618/750 [00:08<00:01, 84.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0481 Train Accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 718/750 [00:09<00:00, 88.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Train Loss: 0.0121 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [00:10<00:00, 72.45it/s]\n",
            "100%|██████████| 1500/1500 [00:08<00:00, 180.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 1] Epoch: 10 Valid Accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "  for idx, data in enumerate(tqdm(train_dataloader)):\n",
        "    images, labels = data\n",
        "\n",
        "    labels = labels.numpy()\n",
        "    output = model.predict(images)\n",
        "    loss = model.loss(output, labels)\n",
        "    grads = model.gradient()\n",
        "    optimizer.step(model.params, grads)\n",
        "    train_acc = model.accuracy(output, labels)\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print(\"[Model 1] Epoch: {} Train Loss: {:.4f} Train Accuracy: {:.4f}\".format(epoch+1, loss, train_acc))\n",
        "\n",
        "  for data in tqdm(val_dataloader):\n",
        "    images, labels = data\n",
        "    labels = labels.numpy()\n",
        "    output = model.predict(images)\n",
        "    val_acc = model.accuracy(output, labels)\n",
        "\n",
        "  print(\"[Model 1] Epoch: {} Valid Accuracy: {:.4f}\".format(epoch+1, val_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVAtckopR14"
      },
      "source": [
        "# Part III. Barebones PyTorch\n",
        "PyTorch는 모델 아키텍처를 편리하게 정의하는 데 도움이 되는 하이레벨 API와 함께 제공되며, 이 튜토리얼의 Part IV에서는 이를 다룰 것입니다. 이 섹션에서는 autograd engine을 더 잘 이해하기 위해 Barebones PyTorch 요소부터 시작하겠습니다. 이 연습을 마치면 하이레벨 모델 API를 더 잘 이해하게 될 것입니다.\n",
        "\n",
        "두 개의 숨겨진 레이어가 있고 MNIST 분류를 위한 bias가 없는 간단한 Fully-connected ReLU 네트워크로 시작하겠습니다. 이 구현은 PyTorch 텐서에서 연산을 사용하여 forward pass를 계산하고 PyTorch autograd를 사용하여 gradient를 계산합니다. 이 예제 이후에 더 어려운 버전을 작성할 것이므로 모든 줄을 이해하는 것이 중요합니다.\n",
        "\n",
        "`requires_grad = True`로 PyTorch 텐서를 생성하면 해당 텐서를 포함하는 연산은 값만 계산하는 것이 아니라 백그라운드에서 계산 그래프를 구축하여 그래프를 통해 쉽게 역전파하여 다운스트림 손실에 대한 일부 텐서의 기울기를 계산할 수 있게 해줍니다. 구체적으로 x가 `x.requires_grad == True`인 텐서인 경우, 역전파 후 `x.grad`는 마지막에 scalar loss에 대한 x의 기울기를 보유하는 또 다른 텐서가 될 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpnPI6lVpR16"
      },
      "source": [
        "### 문제 10 : Barebones PyTorch: Two-Layer Network\n",
        "\n",
        "여기에서는 이미지 데이터 배치에 대해 완전히 연결된 2계층 ReLU 네트워크의 포워드 패스를 수행하는 함수 `two_layer_fc`를 정의합니다. 포워드 패스를 정의한 후에는 네트워크를 통해 0을 실행하여 충돌이 발생하지 않는지, 올바른 모양의 출력을 생성하는지 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg43S8B0pR16",
        "outputId": "dfc773c4-f5cf-4a36-8ac2-67f4af545e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "def flatten(x):\n",
        "    N = x.shape[0] # 데이터의 개수(배치 사이즈)를 먼저 파악한다.\n",
        "    return x.view(N, -1) # N행은 유지하고, 나머지 차원(C,H,W)은 곱해서 하나로 합친다.\n",
        "\n",
        "def two_layer_fc(x, params):\n",
        "    \"\"\"\n",
        "    A fully-connected neural networks; the architecture is:\n",
        "    NN is fully connected -> ReLU -> fully connected layer.\n",
        "    Note that this function only defines the forward pass;\n",
        "    PyTorch will take care of the backward pass for us.\n",
        "\n",
        "    The input to the network will be a minibatch of data, of shape\n",
        "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
        "    and the output layer will produce scores for C classes.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
        "      input data.\n",
        "    - params: A list [w1, w2] of PyTorch Tensors giving weights for the network;\n",
        "      w1 has shape (D, H) and w2 has shape (H, C).\n",
        "\n",
        "    Returns:\n",
        "    - scores: A PyTorch Tensor of shape (N, C) giving classification scores for\n",
        "      the input data x.\n",
        "    \"\"\"\n",
        "    # first we flatten the image\n",
        "    x = flatten(x)  # shape: [batch_size, C x H x W]\n",
        "\n",
        "    w1, w2 = params\n",
        "\n",
        "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
        "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
        "    # PyTorch to build a computational graph, allowing automatic computation of\n",
        "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
        "    # don't need to keep references to intermediate values.\n",
        "    # you can also use `.clamp(min=0)`, equivalent to F.relu()\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: get model socres using w1, w2 and x                                    #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    scores = x @ w1\n",
        "    scores = scores.clamp(min=0)\n",
        "    scores = scores @ w2\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def two_layer_fc_test():\n",
        "    hidden_layer_size = 42\n",
        "    x = torch.zeros((64, 50), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
        "    w1 = torch.zeros((50, hidden_layer_size), dtype=dtype)\n",
        "    w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype)\n",
        "    scores = two_layer_fc(x, [w1, w2])\n",
        "    print(scores.size())  # you should see [64, 10]\n",
        "\n",
        "two_layer_fc_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxzynJspR19"
      },
      "source": [
        "### Barebones PyTorch: Initialization\n",
        "모델의 가중치 행렬을 초기화하는 몇 가지 유틸리티 메서드를 작성해 보겠습니다.\n",
        "\n",
        "- `random_weight(shape)`는 Kaiming normalization 방법으로 가중치 텐서를 초기화합니다.\n",
        "- `zero_weight(shape)`는 모든 0으로 가중치 텐서를 초기화합니다. 바이어스 매개변수를 인스턴스화할 때 유용합니다.\n",
        "\n",
        "`random_weight` 함수는 Kaiming normal initialization 방법을 사용합니다:\n",
        "\n",
        "He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UXkgnTzMpR19"
      },
      "outputs": [],
      "source": [
        "def random_weight(shape):\n",
        "    \"\"\"\n",
        "    Create random Tensors for weights; setting requires_grad=True means that we\n",
        "    want to compute gradients for these Tensors during the backward pass.\n",
        "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
        "    \"\"\"\n",
        "    if len(shape) == 2:  # FC weight\n",
        "        fan_in = shape[0]\n",
        "    else:\n",
        "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
        "    # randn is standard normal distribution generator.\n",
        "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
        "    w.requires_grad = True\n",
        "    return w\n",
        "\n",
        "def zero_weight(shape):\n",
        "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "# create a weight of shape [3 x 5]\n",
        "# you should see the type `torch.cuda.FloatTensor` if you use GPU.\n",
        "# Otherwise it should be `torch.FloatTensor`\n",
        "random_weight((3, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z1t3y6YpR19"
      },
      "source": [
        "### Barebones PyTorch: Check Accuracy\n",
        "모델을 훈련할 때 다음 함수를 사용하여 훈련 또는 검증 세트에서 모델의 정확도를 확인합니다.\n",
        "\n",
        "정확도를 확인할 때 기울기를 계산할 필요가 없으므로 점수를 계산할 때 PyTorch가 계산 그래프를 만들 필요가 없습니다. 그래프가 생성되는 것을 방지하기 위해 `torch.no_grad()` context manager에서 계산 범위를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jre5tpX2pR1-"
      },
      "outputs": [],
      "source": [
        "def check_accuracy_part2(loader, model_fn, params, istrain = True):\n",
        "    \"\"\"\n",
        "    Check the accuracy of a classification model.\n",
        "\n",
        "    Inputs:\n",
        "    - loader: A DataLoader for the data split we want to check\n",
        "    - model_fn: A function that performs the forward pass of the model,\n",
        "      with the signature scores = model_fn(x, params)\n",
        "    - params: List of PyTorch Tensors giving parameters of the model\n",
        "\n",
        "    Returns: Nothing, but prints the accuracy of the model\n",
        "    \"\"\"\n",
        "    split = 'val' if istrain else 'test'\n",
        "    print('Checking accuracy on the %s set' % split)\n",
        "    num_correct, num_samples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.int64)\n",
        "            scores = model_fn(x, params)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNMOvc7pR1-"
      },
      "source": [
        "### BareBones PyTorch: Training Loop\n",
        "이제 네트워크를 훈련하기 위한 basic training loop를 설정할 수 있습니다. momentum 없이 Stochastic gradient descent를 사용하여 모델을 훈련할 것입니다. 여기서는 `torch.functional.cross_entropy`를 사용하여 loss를 계산할 것입니다(http://pytorch.org/docs/stable/nn.html#cross-entropy).\n",
        "\n",
        "training loop는 신경망 함수, 초기화된 매개변수 목록(예제에서는 `[w1, w2]`), 학습 속도를 입력으로 받습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fyt2Boy_pR1-"
      },
      "outputs": [],
      "source": [
        "def train_part2(model_fn, params, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a model on MNIST.\n",
        "\n",
        "    Inputs:\n",
        "    - model_fn: A Python function that performs the forward pass of the model.\n",
        "      It should have the signature scores = model_fn(x, params) where x is a\n",
        "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
        "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
        "      scores for the elements in x.\n",
        "    - params: List of PyTorch Tensors giving weights for the model\n",
        "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
        "\n",
        "    Returns: Nothing\n",
        "    \"\"\"\n",
        "    for t, (x, y) in enumerate(train_dataloader):\n",
        "        # Move the data to the proper device (GPU or CPU)\n",
        "        x = x.to(device=device, dtype=dtype)\n",
        "        y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "        # Forward pass: compute scores and loss\n",
        "        scores = model_fn(x, params)\n",
        "        loss = F.cross_entropy(scores, y)\n",
        "\n",
        "        # Backward pass: PyTorch figures out which Tensors in the computational\n",
        "        # graph has requires_grad=True and uses backpropagation to compute the\n",
        "        # gradient of the loss with respect to these Tensors, and stores the\n",
        "        # gradients in the .grad attribute of each Tensor.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters. We don't want to backpropagate through the\n",
        "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
        "        # context manager to prevent a computational graph from being built.\n",
        "        with torch.no_grad():\n",
        "            for w in params:\n",
        "                w -= learning_rate * w.grad\n",
        "\n",
        "                # Manually zero the gradients after running the backward pass\n",
        "                w.grad.zero_()\n",
        "\n",
        "        if t % print_every == 0:\n",
        "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
        "            check_accuracy_part2(val_dataloader, model_fn, params)\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhZfY1PzpR1_"
      },
      "source": [
        "### BareBones PyTorch: Train a Two-Layer Network\n",
        "이제 training loop를 실행할 준비가 되었습니다. fully connected weights인 `w1`과 `w2`에 대한 Tensor를 명시적으로 할당해야 합니다.\n",
        "\n",
        "MNIST의 각 minibatch에는 64개의 예가 있으므로 텐서 모양은 `[64, 1, 28, 28]`입니다.\n",
        "\n",
        "flatten 후 `x` 모양은 `[64, 1 * 28 * 28]`가 되어야 합니다. 이것이 `w1`의 첫 번째 차원 크기가 됩니다.\n",
        "`w1`의 두 번째 차원은 hidden layer size이며, 이는 또한 `w2`의 첫 번째 차원이 됩니다.\n",
        "\n",
        "마지막으로 네트워크의 출력은 10개의 클래스에 대한 확률 분포를 나타내는 10차원 벡터입니다.\n",
        "\n",
        "hyperparameter를 조정할 필요는 없지만 한 회기 동안 훈련한 후에는 40% 이상의 정확도를 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OVhigckkpR1_"
      },
      "outputs": [],
      "source": [
        "hidden_layer_size = 64\n",
        "learning_rate = 1e-2\n",
        "\n",
        "w1 = random_weight((1 * 28 * 28, hidden_layer_size))\n",
        "w2 = random_weight((hidden_layer_size, 10))\n",
        "\n",
        "train_part2(two_layer_fc, [w1, w2], learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpzU23oApR2A"
      },
      "source": [
        "# Part IV. PyTorch Module API\n",
        "\n",
        "Barebone PyTorch에서는 모든 Parameter tensors를 수작업으로 추적해야 합니다. 이는 몇 개의 텐서가 있는 소규모 네트워크에서는 괜찮지만, 대규모 네트워크에서 수십 또는 수백 개의 텐서를 추적하는 것은 매우 불편하고 오류가 발생하기 쉽습니다.\n",
        "\n",
        "PyTorch는 임의의 네트워크 아키텍처를 정의하는 동시에 학습 가능한 모든 파라미터를 추적할 수 있도록 `nn.Module` API를 제공합니다. Part II에서는 SGD를 직접 구현해 보았습니다. PyTorch는 또한 RMSProp, Adagrad, Adam과 같은 모든 일반적인 Optimizer를 구현하는 `torch.optim` 패키지를 제공합니다. 심지어 L-BFGS와 같은 대략적인 2차 방법도 지원합니다! 각 옵티마이저의 정확한 사양은 [문서](http://pytorch.org/docs/master/optim.html)를 참고하세요.\n",
        "\n",
        "모듈 API를 사용하려면 아래 단계를 따르세요:\n",
        "\n",
        "1. 서브클래스 `nn.Module`. 네트워크 클래스에 `TwoLayerFC`와 같은 직관적인 이름을 지정합니다.\n",
        "\n",
        "2. 생성자 `__init__()`에서 필요한 모든 레이어를 클래스 속성으로 정의합니다. `nn.Linear` 및 `nn.Conv2d`와 같은 레이어 객체는 그 자체로 `nn.Module` 서브클래스이며 학습 가능한 파라미터를 포함하므로 원시 텐서를 직접 인스턴스화할 필요가 없습니다. `nn.Module`이 이러한 내부 파라미터를 추적합니다. 수십 개의 내장 레이어에 대해 자세히 알아보려면 [문서](http://pytorch.org/docs/master/nn.html)를 참조하세요. **경고**: `super().__init__()`를 먼저 호출하는 것을 잊지 마세요!\n",
        "\n",
        "3. `forward()` 메서드에서 네트워크의 *연결성*을 정의합니다. 텐서를 입력으로 받고 \"변환된\" 텐서를 출력하는 함수 호출로 `__init__`에 정의된 속성을 사용해야 합니다. `forward()`에서 학습 가능한 매개변수가 있는 새 레이어를 생성하지 마세요! 모든 매개변수는 `__init__`에서 미리 선언해야 합니다.\n",
        "\n",
        "모듈 서브클래스를 정의한 후에는 객체로 인스턴스화하여 Part II의 NN 전달 함수처럼 호출할 수 있습니다.\n",
        "\n",
        "### Module API: Two-Layer Network\n",
        "다음은 fully connected 2계층 네트워크의 구체적인 예입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WSrvts7LpR2A"
      },
      "outputs": [],
      "source": [
        "class TwoLayerFC(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        # assign layer objects to class attributes\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # nn.init package contains convenient initialization methods\n",
        "        # http://pytorch.org/docs/master/nn.html#torch-nn-init\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward always defines connectivity\n",
        "        x = flatten(x)\n",
        "        scores = self.fc2(F.relu(self.fc1(x)))\n",
        "        return scores\n",
        "\n",
        "def test_TwoLayerFC():\n",
        "    input_size = 50\n",
        "    x = torch.zeros((64, input_size), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
        "    model = TwoLayerFC(input_size, 42, 10)\n",
        "    scores = model(x)\n",
        "    print(scores.size())  # you should see [64, 10]\n",
        "test_TwoLayerFC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3CDip-7pR2B"
      },
      "source": [
        "### Module API: Check Accuracy\n",
        "검증 또는 테스트 세트가 주어지면 신경망의 분류 정확도를 확인할 수 있습니다.\n",
        "\n",
        "이 버전은 Part II의 버전과 약간 다릅니다. 더 이상 매개 변수를 수동으로 전달하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7PYAZQ1DpR2B"
      },
      "outputs": [],
      "source": [
        "def check_accuracy_part34(loader, model, istrain = True):\n",
        "    if istrain:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb6341vOpR2B"
      },
      "source": [
        "### 문제 11. Module API: Training Loop\n",
        "또한 약간 다른 training loop를 사용합니다. 가중치 값을 직접 업데이트하는 대신, Optimization 알고리즘의 개념을 추상화하고 신경망 최적화에 일반적으로 사용되는 대부분의 알고리즘 구현을 제공하는 `torch.optim` 패키지의 Optimizer 객체를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lY-B6AXbpR2B"
      },
      "outputs": [],
      "source": [
        "def train_part34(model, optimizer, epochs=2):\n",
        "    \"\"\"\n",
        "    Train a model on MNIST using the PyTorch Module API.\n",
        "\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(train_dataloader):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ################################################################################\n",
        "            # TODO: get model output and calculate cross entropy loss                      #\n",
        "            ################################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            ################################################################################\n",
        "            #                                 END OF YOUR CODE                             #\n",
        "            ################################################################################\n",
        "\n",
        "\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
        "                check_accuracy_part34(val_dataloader, model)\n",
        "                print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMFEsm-OpR2C"
      },
      "source": [
        "### Module API: Train a Two-Layer Network\n",
        "이제 training loop를 실행할 준비가 되었습니다. Part II와 달리 이번에는 Parameter tensors를 더 이상 명시적으로 할당하지 않습니다.\n",
        "\n",
        "입력 크기, 숨겨진 레이어 크기, 클래스 수(즉, 출력 크기)를 `TwoLayerFC`의 생성자에 전달하기만 하면 됩니다.\n",
        "\n",
        "또한 `TwoLayerFC` 내에서 학습 가능한 모든 파라미터를 추적하는 옵티마이저를 정의해야 합니다.\n",
        "\n",
        "하이퍼파라미터를 조정할 필요는 없지만, 한 epoch 동안 학습한 후 40% 이상의 모델 정확도를 볼 수 있어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "TCwaZfJTpR2C"
      },
      "outputs": [],
      "source": [
        "hidden_layer_size = 64\n",
        "learning_rate = 1e-2\n",
        "model = TwoLayerFC(784, hidden_layer_size, 10)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_part34(model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsFqIP43pR2D"
      },
      "source": [
        "# 문제 12. Part V. PyTorch Sequential API\n",
        "\n",
        "Part IV에서는 임의의 학습 가능한 레이어와 그 연결성을 정의할 수 있는 PyTorch 모듈 API를 소개했습니다.\n",
        "\n",
        "Feed forward layers과 같은 간단한 모델의 경우, `nn.Module` 서브클래스를 생성하고, `__init__`에서 클래스 속성에 레이어를 할당하고, `forward()`에서 각 레이어를 하나씩 호출하는 3단계를 거쳐야 합니다. 더 편리한 방법이 있을까요?\n",
        "\n",
        "다행히도 PyTorch에서는 위의 단계를 하나로 합친 `nn.Sequential`이라는 컨테이너 모듈을 제공합니다. feed forward stacks보다 더 복잡한 topology를 지정할 수 없기 때문에 `nn.Module`만큼 유연하지는 않지만, 많은 사용 사례에 충분합니다.\n",
        "\n",
        "### Sequential API: 3계층 네트워크\n",
        "`nn.Sequential`을 사용하여 3계층 fully connected 네트워크 예제를 다시 작성하고 위에서 정의한 training loop를 사용하여 트레이닝하는 방법을 살펴봅시다.\n",
        "\n",
        "\n",
        "1. output dimension이 256인 fully-coonected layer\n",
        "2. ReLU\n",
        "3. output dimension이 64인 fully-coonected layer\n",
        "4. ReLU\n",
        "5. 10개의 클래스에 대한 점수를 계산하기 위한 fully-coonected layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hTN-_2OmpR2D",
        "outputId": "1b20686e-c274-49fb-b8ba-0e269ff96050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 2.3116\n",
            "Checking accuracy on validation set\n",
            "Got 9444 / 48000 correct (19.68)\n",
            "\n",
            "Iteration 100, loss = 1.4907\n",
            "Checking accuracy on validation set\n",
            "Got 33077 / 48000 correct (68.91)\n",
            "\n",
            "Iteration 200, loss = 0.8580\n",
            "Checking accuracy on validation set\n",
            "Got 39025 / 48000 correct (81.30)\n",
            "\n",
            "Iteration 300, loss = 0.5991\n",
            "Checking accuracy on validation set\n",
            "Got 40930 / 48000 correct (85.27)\n",
            "\n",
            "Iteration 400, loss = 0.4055\n",
            "Checking accuracy on validation set\n",
            "Got 41661 / 48000 correct (86.79)\n",
            "\n",
            "Iteration 500, loss = 0.4748\n",
            "Checking accuracy on validation set\n",
            "Got 42088 / 48000 correct (87.68)\n",
            "\n",
            "Iteration 600, loss = 0.2677\n",
            "Checking accuracy on validation set\n",
            "Got 42388 / 48000 correct (88.31)\n",
            "\n",
            "Iteration 700, loss = 0.3771\n",
            "Checking accuracy on validation set\n",
            "Got 42683 / 48000 correct (88.92)\n",
            "\n",
            "Iteration 0, loss = 0.2668\n",
            "Checking accuracy on validation set\n",
            "Got 42835 / 48000 correct (89.24)\n",
            "\n",
            "Iteration 100, loss = 0.3849\n",
            "Checking accuracy on validation set\n",
            "Got 42998 / 48000 correct (89.58)\n",
            "\n",
            "Iteration 200, loss = 0.3713\n",
            "Checking accuracy on validation set\n",
            "Got 43144 / 48000 correct (89.88)\n",
            "\n",
            "Iteration 300, loss = 0.1912\n",
            "Checking accuracy on validation set\n",
            "Got 43403 / 48000 correct (90.42)\n",
            "\n",
            "Iteration 400, loss = 0.3141\n",
            "Checking accuracy on validation set\n",
            "Got 43360 / 48000 correct (90.33)\n",
            "\n",
            "Iteration 500, loss = 0.2274\n",
            "Checking accuracy on validation set\n",
            "Got 43503 / 48000 correct (90.63)\n",
            "\n",
            "Iteration 600, loss = 0.2468\n",
            "Checking accuracy on validation set\n",
            "Got 43560 / 48000 correct (90.75)\n",
            "\n",
            "Iteration 700, loss = 0.2004\n",
            "Checking accuracy on validation set\n",
            "Got 43798 / 48000 correct (91.25)\n",
            "\n",
            "Iteration 0, loss = 0.2502\n",
            "Checking accuracy on validation set\n",
            "Got 43428 / 48000 correct (90.48)\n",
            "\n",
            "Iteration 100, loss = 0.3127\n",
            "Checking accuracy on validation set\n",
            "Got 43796 / 48000 correct (91.24)\n",
            "\n",
            "Iteration 200, loss = 0.3296\n",
            "Checking accuracy on validation set\n",
            "Got 43906 / 48000 correct (91.47)\n",
            "\n",
            "Iteration 300, loss = 0.3340\n",
            "Checking accuracy on validation set\n",
            "Got 44024 / 48000 correct (91.72)\n",
            "\n",
            "Iteration 400, loss = 0.2385\n",
            "Checking accuracy on validation set\n",
            "Got 43986 / 48000 correct (91.64)\n",
            "\n",
            "Iteration 500, loss = 0.3149\n",
            "Checking accuracy on validation set\n",
            "Got 44254 / 48000 correct (92.20)\n",
            "\n",
            "Iteration 600, loss = 0.3100\n",
            "Checking accuracy on validation set\n",
            "Got 44238 / 48000 correct (92.16)\n",
            "\n",
            "Iteration 700, loss = 0.2617\n",
            "Checking accuracy on validation set\n",
            "Got 44187 / 48000 correct (92.06)\n",
            "\n",
            "Iteration 0, loss = 0.2571\n",
            "Checking accuracy on validation set\n",
            "Got 44451 / 48000 correct (92.61)\n",
            "\n",
            "Iteration 100, loss = 0.1739\n",
            "Checking accuracy on validation set\n",
            "Got 44338 / 48000 correct (92.37)\n",
            "\n",
            "Iteration 200, loss = 0.3598\n",
            "Checking accuracy on validation set\n",
            "Got 44380 / 48000 correct (92.46)\n",
            "\n",
            "Iteration 300, loss = 0.2474\n",
            "Checking accuracy on validation set\n",
            "Got 44469 / 48000 correct (92.64)\n",
            "\n",
            "Iteration 400, loss = 0.2280\n",
            "Checking accuracy on validation set\n",
            "Got 44615 / 48000 correct (92.95)\n",
            "\n",
            "Iteration 500, loss = 0.2173\n",
            "Checking accuracy on validation set\n",
            "Got 44623 / 48000 correct (92.96)\n",
            "\n",
            "Iteration 600, loss = 0.2079\n",
            "Checking accuracy on validation set\n",
            "Got 44807 / 48000 correct (93.35)\n",
            "\n",
            "Iteration 700, loss = 0.1343\n",
            "Checking accuracy on validation set\n",
            "Got 44822 / 48000 correct (93.38)\n",
            "\n",
            "Iteration 0, loss = 0.3515\n",
            "Checking accuracy on validation set\n",
            "Got 44728 / 48000 correct (93.18)\n",
            "\n",
            "Iteration 100, loss = 0.2804\n",
            "Checking accuracy on validation set\n",
            "Got 44651 / 48000 correct (93.02)\n",
            "\n",
            "Iteration 200, loss = 0.1273\n",
            "Checking accuracy on validation set\n",
            "Got 44985 / 48000 correct (93.72)\n",
            "\n",
            "Iteration 300, loss = 0.1205\n",
            "Checking accuracy on validation set\n",
            "Got 45003 / 48000 correct (93.76)\n",
            "\n",
            "Iteration 400, loss = 0.3906\n",
            "Checking accuracy on validation set\n",
            "Got 45058 / 48000 correct (93.87)\n",
            "\n",
            "Iteration 500, loss = 0.1358\n",
            "Checking accuracy on validation set\n",
            "Got 45109 / 48000 correct (93.98)\n",
            "\n",
            "Iteration 600, loss = 0.2016\n",
            "Checking accuracy on validation set\n",
            "Got 45214 / 48000 correct (94.20)\n",
            "\n",
            "Iteration 700, loss = 0.2759\n",
            "Checking accuracy on validation set\n",
            "Got 45149 / 48000 correct (94.06)\n",
            "\n",
            "Iteration 0, loss = 0.2924\n",
            "Checking accuracy on validation set\n",
            "Got 45097 / 48000 correct (93.95)\n",
            "\n",
            "Iteration 100, loss = 0.1000\n",
            "Checking accuracy on validation set\n",
            "Got 45287 / 48000 correct (94.35)\n",
            "\n",
            "Iteration 200, loss = 0.1784\n",
            "Checking accuracy on validation set\n",
            "Got 45208 / 48000 correct (94.18)\n",
            "\n",
            "Iteration 300, loss = 0.1790\n",
            "Checking accuracy on validation set\n",
            "Got 45338 / 48000 correct (94.45)\n",
            "\n",
            "Iteration 400, loss = 0.2154\n",
            "Checking accuracy on validation set\n",
            "Got 45392 / 48000 correct (94.57)\n",
            "\n",
            "Iteration 500, loss = 0.2706\n",
            "Checking accuracy on validation set\n",
            "Got 45452 / 48000 correct (94.69)\n",
            "\n",
            "Iteration 600, loss = 0.2158\n",
            "Checking accuracy on validation set\n",
            "Got 45208 / 48000 correct (94.18)\n",
            "\n",
            "Iteration 700, loss = 0.1819\n",
            "Checking accuracy on validation set\n",
            "Got 45587 / 48000 correct (94.97)\n",
            "\n",
            "Iteration 0, loss = 0.2390\n",
            "Checking accuracy on validation set\n",
            "Got 45532 / 48000 correct (94.86)\n",
            "\n",
            "Iteration 100, loss = 0.1453\n",
            "Checking accuracy on validation set\n",
            "Got 45546 / 48000 correct (94.89)\n",
            "\n",
            "Iteration 200, loss = 0.1806\n",
            "Checking accuracy on validation set\n",
            "Got 45454 / 48000 correct (94.70)\n",
            "\n",
            "Iteration 300, loss = 0.3092\n",
            "Checking accuracy on validation set\n",
            "Got 45502 / 48000 correct (94.80)\n",
            "\n",
            "Iteration 400, loss = 0.0714\n",
            "Checking accuracy on validation set\n",
            "Got 45590 / 48000 correct (94.98)\n",
            "\n",
            "Iteration 500, loss = 0.1495\n",
            "Checking accuracy on validation set\n",
            "Got 45690 / 48000 correct (95.19)\n",
            "\n",
            "Iteration 600, loss = 0.2309\n",
            "Checking accuracy on validation set\n",
            "Got 45731 / 48000 correct (95.27)\n",
            "\n",
            "Iteration 700, loss = 0.0422\n",
            "Checking accuracy on validation set\n",
            "Got 45731 / 48000 correct (95.27)\n",
            "\n",
            "Iteration 0, loss = 0.1960\n",
            "Checking accuracy on validation set\n",
            "Got 45780 / 48000 correct (95.38)\n",
            "\n",
            "Iteration 100, loss = 0.0932\n",
            "Checking accuracy on validation set\n",
            "Got 45695 / 48000 correct (95.20)\n",
            "\n",
            "Iteration 200, loss = 0.0736\n",
            "Checking accuracy on validation set\n",
            "Got 45822 / 48000 correct (95.46)\n",
            "\n",
            "Iteration 300, loss = 0.1051\n",
            "Checking accuracy on validation set\n",
            "Got 45773 / 48000 correct (95.36)\n",
            "\n",
            "Iteration 400, loss = 0.1622\n",
            "Checking accuracy on validation set\n",
            "Got 45885 / 48000 correct (95.59)\n",
            "\n",
            "Iteration 500, loss = 0.1075\n",
            "Checking accuracy on validation set\n",
            "Got 45957 / 48000 correct (95.74)\n",
            "\n",
            "Iteration 600, loss = 0.2266\n",
            "Checking accuracy on validation set\n",
            "Got 45945 / 48000 correct (95.72)\n",
            "\n",
            "Iteration 700, loss = 0.0563\n",
            "Checking accuracy on validation set\n",
            "Got 46001 / 48000 correct (95.84)\n",
            "\n",
            "Iteration 0, loss = 0.0732\n",
            "Checking accuracy on validation set\n",
            "Got 45951 / 48000 correct (95.73)\n",
            "\n",
            "Iteration 100, loss = 0.0621\n",
            "Checking accuracy on validation set\n",
            "Got 46032 / 48000 correct (95.90)\n",
            "\n",
            "Iteration 200, loss = 0.0967\n",
            "Checking accuracy on validation set\n",
            "Got 45913 / 48000 correct (95.65)\n",
            "\n",
            "Iteration 300, loss = 0.0903\n",
            "Checking accuracy on validation set\n",
            "Got 46019 / 48000 correct (95.87)\n",
            "\n",
            "Iteration 400, loss = 0.0885\n",
            "Checking accuracy on validation set\n",
            "Got 45964 / 48000 correct (95.76)\n",
            "\n",
            "Iteration 500, loss = 0.0654\n",
            "Checking accuracy on validation set\n",
            "Got 46017 / 48000 correct (95.87)\n",
            "\n",
            "Iteration 600, loss = 0.1447\n",
            "Checking accuracy on validation set\n",
            "Got 46172 / 48000 correct (96.19)\n",
            "\n",
            "Iteration 700, loss = 0.0619\n",
            "Checking accuracy on validation set\n",
            "Got 46069 / 48000 correct (95.98)\n",
            "\n",
            "Iteration 0, loss = 0.1623\n",
            "Checking accuracy on validation set\n",
            "Got 46164 / 48000 correct (96.17)\n",
            "\n",
            "Iteration 100, loss = 0.1385\n",
            "Checking accuracy on validation set\n",
            "Got 46163 / 48000 correct (96.17)\n",
            "\n",
            "Iteration 200, loss = 0.1846\n",
            "Checking accuracy on validation set\n",
            "Got 46177 / 48000 correct (96.20)\n",
            "\n",
            "Iteration 300, loss = 0.1532\n",
            "Checking accuracy on validation set\n",
            "Got 46216 / 48000 correct (96.28)\n",
            "\n",
            "Iteration 400, loss = 0.1534\n",
            "Checking accuracy on validation set\n",
            "Got 46294 / 48000 correct (96.45)\n",
            "\n",
            "Iteration 500, loss = 0.1801\n",
            "Checking accuracy on validation set\n",
            "Got 46347 / 48000 correct (96.56)\n",
            "\n",
            "Iteration 600, loss = 0.1650\n",
            "Checking accuracy on validation set\n",
            "Got 46333 / 48000 correct (96.53)\n",
            "\n",
            "Iteration 700, loss = 0.0683\n",
            "Checking accuracy on validation set\n",
            "Got 46346 / 48000 correct (96.55)\n",
            "\n",
            "Iteration 0, loss = 0.1683\n",
            "Checking accuracy on validation set\n",
            "Got 46318 / 48000 correct (96.50)\n",
            "\n",
            "Iteration 100, loss = 0.1838\n",
            "Checking accuracy on validation set\n",
            "Got 46315 / 48000 correct (96.49)\n",
            "\n",
            "Iteration 200, loss = 0.0935\n",
            "Checking accuracy on validation set\n",
            "Got 46426 / 48000 correct (96.72)\n",
            "\n",
            "Iteration 300, loss = 0.0581\n",
            "Checking accuracy on validation set\n",
            "Got 46431 / 48000 correct (96.73)\n",
            "\n",
            "Iteration 400, loss = 0.1548\n",
            "Checking accuracy on validation set\n",
            "Got 46440 / 48000 correct (96.75)\n",
            "\n",
            "Iteration 500, loss = 0.1904\n",
            "Checking accuracy on validation set\n",
            "Got 46434 / 48000 correct (96.74)\n",
            "\n",
            "Iteration 600, loss = 0.0705\n",
            "Checking accuracy on validation set\n",
            "Got 46331 / 48000 correct (96.52)\n",
            "\n",
            "Iteration 700, loss = 0.1203\n",
            "Checking accuracy on validation set\n",
            "Got 46425 / 48000 correct (96.72)\n",
            "\n",
            "Iteration 0, loss = 0.1071\n",
            "Checking accuracy on validation set\n",
            "Got 46459 / 48000 correct (96.79)\n",
            "\n",
            "Iteration 100, loss = 0.0793\n",
            "Checking accuracy on validation set\n",
            "Got 46490 / 48000 correct (96.85)\n",
            "\n",
            "Iteration 200, loss = 0.0939\n",
            "Checking accuracy on validation set\n",
            "Got 46335 / 48000 correct (96.53)\n",
            "\n",
            "Iteration 300, loss = 0.1053\n",
            "Checking accuracy on validation set\n",
            "Got 46494 / 48000 correct (96.86)\n",
            "\n",
            "Iteration 400, loss = 0.0919\n",
            "Checking accuracy on validation set\n",
            "Got 46536 / 48000 correct (96.95)\n",
            "\n",
            "Iteration 500, loss = 0.0458\n",
            "Checking accuracy on validation set\n",
            "Got 46494 / 48000 correct (96.86)\n",
            "\n",
            "Iteration 600, loss = 0.1307\n",
            "Checking accuracy on validation set\n",
            "Got 46561 / 48000 correct (97.00)\n",
            "\n",
            "Iteration 700, loss = 0.2299\n",
            "Checking accuracy on validation set\n",
            "Got 46635 / 48000 correct (97.16)\n",
            "\n",
            "Iteration 0, loss = 0.0723\n",
            "Checking accuracy on validation set\n",
            "Got 46630 / 48000 correct (97.15)\n",
            "\n",
            "Iteration 100, loss = 0.1372\n",
            "Checking accuracy on validation set\n",
            "Got 46615 / 48000 correct (97.11)\n",
            "\n",
            "Iteration 200, loss = 0.1243\n",
            "Checking accuracy on validation set\n",
            "Got 46638 / 48000 correct (97.16)\n",
            "\n",
            "Iteration 300, loss = 0.1224\n",
            "Checking accuracy on validation set\n",
            "Got 46713 / 48000 correct (97.32)\n",
            "\n",
            "Iteration 400, loss = 0.0661\n",
            "Checking accuracy on validation set\n",
            "Got 46628 / 48000 correct (97.14)\n",
            "\n",
            "Iteration 500, loss = 0.0836\n",
            "Checking accuracy on validation set\n",
            "Got 46717 / 48000 correct (97.33)\n",
            "\n",
            "Iteration 600, loss = 0.0344\n",
            "Checking accuracy on validation set\n",
            "Got 46633 / 48000 correct (97.15)\n",
            "\n",
            "Iteration 700, loss = 0.0978\n",
            "Checking accuracy on validation set\n",
            "Got 46738 / 48000 correct (97.37)\n",
            "\n",
            "Iteration 0, loss = 0.0999\n",
            "Checking accuracy on validation set\n",
            "Got 46672 / 48000 correct (97.23)\n",
            "\n",
            "Iteration 100, loss = 0.0396\n",
            "Checking accuracy on validation set\n",
            "Got 46656 / 48000 correct (97.20)\n",
            "\n",
            "Iteration 200, loss = 0.0946\n",
            "Checking accuracy on validation set\n",
            "Got 46738 / 48000 correct (97.37)\n",
            "\n",
            "Iteration 300, loss = 0.1589\n",
            "Checking accuracy on validation set\n",
            "Got 46713 / 48000 correct (97.32)\n",
            "\n",
            "Iteration 400, loss = 0.1596\n",
            "Checking accuracy on validation set\n",
            "Got 46737 / 48000 correct (97.37)\n",
            "\n",
            "Iteration 500, loss = 0.0956\n",
            "Checking accuracy on validation set\n",
            "Got 46742 / 48000 correct (97.38)\n",
            "\n",
            "Iteration 600, loss = 0.0817\n",
            "Checking accuracy on validation set\n",
            "Got 46817 / 48000 correct (97.54)\n",
            "\n",
            "Iteration 700, loss = 0.0314\n",
            "Checking accuracy on validation set\n",
            "Got 46865 / 48000 correct (97.64)\n",
            "\n",
            "Iteration 0, loss = 0.0270\n",
            "Checking accuracy on validation set\n",
            "Got 46806 / 48000 correct (97.51)\n",
            "\n",
            "Iteration 100, loss = 0.0377\n",
            "Checking accuracy on validation set\n",
            "Got 46824 / 48000 correct (97.55)\n",
            "\n",
            "Iteration 200, loss = 0.0268\n",
            "Checking accuracy on validation set\n",
            "Got 46668 / 48000 correct (97.22)\n",
            "\n",
            "Iteration 300, loss = 0.0211\n",
            "Checking accuracy on validation set\n",
            "Got 46889 / 48000 correct (97.69)\n",
            "\n",
            "Iteration 400, loss = 0.0670\n",
            "Checking accuracy on validation set\n",
            "Got 46901 / 48000 correct (97.71)\n",
            "\n",
            "Iteration 500, loss = 0.0498\n",
            "Checking accuracy on validation set\n",
            "Got 46932 / 48000 correct (97.78)\n",
            "\n",
            "Iteration 600, loss = 0.0679\n",
            "Checking accuracy on validation set\n",
            "Got 46925 / 48000 correct (97.76)\n",
            "\n",
            "Iteration 700, loss = 0.1363\n",
            "Checking accuracy on validation set\n",
            "Got 46869 / 48000 correct (97.64)\n",
            "\n",
            "Iteration 0, loss = 0.0191\n",
            "Checking accuracy on validation set\n",
            "Got 46943 / 48000 correct (97.80)\n",
            "\n",
            "Iteration 100, loss = 0.1930\n",
            "Checking accuracy on validation set\n",
            "Got 46983 / 48000 correct (97.88)\n",
            "\n",
            "Iteration 200, loss = 0.0522\n",
            "Checking accuracy on validation set\n",
            "Got 46968 / 48000 correct (97.85)\n",
            "\n",
            "Iteration 300, loss = 0.0656\n",
            "Checking accuracy on validation set\n",
            "Got 46946 / 48000 correct (97.80)\n",
            "\n",
            "Iteration 400, loss = 0.1722\n",
            "Checking accuracy on validation set\n",
            "Got 46954 / 48000 correct (97.82)\n",
            "\n",
            "Iteration 500, loss = 0.0667\n",
            "Checking accuracy on validation set\n",
            "Got 46931 / 48000 correct (97.77)\n",
            "\n",
            "Iteration 600, loss = 0.0416\n",
            "Checking accuracy on validation set\n",
            "Got 46967 / 48000 correct (97.85)\n",
            "\n",
            "Iteration 700, loss = 0.1080\n",
            "Checking accuracy on validation set\n",
            "Got 46996 / 48000 correct (97.91)\n",
            "\n",
            "Iteration 0, loss = 0.0872\n",
            "Checking accuracy on validation set\n",
            "Got 46909 / 48000 correct (97.73)\n",
            "\n",
            "Iteration 100, loss = 0.0200\n",
            "Checking accuracy on validation set\n",
            "Got 47008 / 48000 correct (97.93)\n",
            "\n",
            "Iteration 200, loss = 0.0361\n",
            "Checking accuracy on validation set\n",
            "Got 47034 / 48000 correct (97.99)\n",
            "\n",
            "Iteration 300, loss = 0.0104\n",
            "Checking accuracy on validation set\n",
            "Got 47088 / 48000 correct (98.10)\n",
            "\n",
            "Iteration 400, loss = 0.1277\n",
            "Checking accuracy on validation set\n",
            "Got 47031 / 48000 correct (97.98)\n",
            "\n",
            "Iteration 500, loss = 0.0824\n",
            "Checking accuracy on validation set\n",
            "Got 47004 / 48000 correct (97.92)\n",
            "\n",
            "Iteration 600, loss = 0.1100\n",
            "Checking accuracy on validation set\n",
            "Got 47081 / 48000 correct (98.09)\n",
            "\n",
            "Iteration 700, loss = 0.1113\n",
            "Checking accuracy on validation set\n",
            "Got 47114 / 48000 correct (98.15)\n",
            "\n",
            "Iteration 0, loss = 0.0912\n",
            "Checking accuracy on validation set\n",
            "Got 47028 / 48000 correct (97.97)\n",
            "\n",
            "Iteration 100, loss = 0.0444\n",
            "Checking accuracy on validation set\n",
            "Got 47093 / 48000 correct (98.11)\n",
            "\n",
            "Iteration 200, loss = 0.0648\n",
            "Checking accuracy on validation set\n",
            "Got 47079 / 48000 correct (98.08)\n",
            "\n",
            "Iteration 300, loss = 0.1129\n",
            "Checking accuracy on validation set\n",
            "Got 47037 / 48000 correct (97.99)\n",
            "\n",
            "Iteration 400, loss = 0.0882\n",
            "Checking accuracy on validation set\n",
            "Got 47075 / 48000 correct (98.07)\n",
            "\n",
            "Iteration 500, loss = 0.1340\n",
            "Checking accuracy on validation set\n",
            "Got 47105 / 48000 correct (98.14)\n",
            "\n",
            "Iteration 600, loss = 0.1504\n",
            "Checking accuracy on validation set\n",
            "Got 47104 / 48000 correct (98.13)\n",
            "\n",
            "Iteration 700, loss = 0.1280\n",
            "Checking accuracy on validation set\n",
            "Got 47140 / 48000 correct (98.21)\n",
            "\n",
            "Iteration 0, loss = 0.0240\n",
            "Checking accuracy on validation set\n",
            "Got 47161 / 48000 correct (98.25)\n",
            "\n",
            "Iteration 100, loss = 0.0840\n",
            "Checking accuracy on validation set\n",
            "Got 47174 / 48000 correct (98.28)\n",
            "\n",
            "Iteration 200, loss = 0.0352\n",
            "Checking accuracy on validation set\n",
            "Got 47162 / 48000 correct (98.25)\n",
            "\n",
            "Iteration 300, loss = 0.0166\n",
            "Checking accuracy on validation set\n",
            "Got 47162 / 48000 correct (98.25)\n",
            "\n",
            "Iteration 400, loss = 0.0219\n",
            "Checking accuracy on validation set\n",
            "Got 47139 / 48000 correct (98.21)\n",
            "\n",
            "Iteration 500, loss = 0.0998\n",
            "Checking accuracy on validation set\n",
            "Got 47181 / 48000 correct (98.29)\n",
            "\n",
            "Iteration 600, loss = 0.0179\n",
            "Checking accuracy on validation set\n",
            "Got 47238 / 48000 correct (98.41)\n",
            "\n",
            "Iteration 700, loss = 0.0393\n",
            "Checking accuracy on validation set\n",
            "Got 47174 / 48000 correct (98.28)\n",
            "\n",
            "Iteration 0, loss = 0.0483\n",
            "Checking accuracy on validation set\n",
            "Got 47214 / 48000 correct (98.36)\n",
            "\n",
            "Iteration 100, loss = 0.0973\n",
            "Checking accuracy on validation set\n",
            "Got 47255 / 48000 correct (98.45)\n",
            "\n",
            "Iteration 200, loss = 0.1154\n",
            "Checking accuracy on validation set\n",
            "Got 47209 / 48000 correct (98.35)\n",
            "\n",
            "Iteration 300, loss = 0.0768\n",
            "Checking accuracy on validation set\n",
            "Got 47245 / 48000 correct (98.43)\n",
            "\n",
            "Iteration 400, loss = 0.0854\n",
            "Checking accuracy on validation set\n",
            "Got 47189 / 48000 correct (98.31)\n",
            "\n",
            "Iteration 500, loss = 0.1192\n",
            "Checking accuracy on validation set\n",
            "Got 47222 / 48000 correct (98.38)\n",
            "\n",
            "Iteration 600, loss = 0.0331\n",
            "Checking accuracy on validation set\n",
            "Got 47299 / 48000 correct (98.54)\n",
            "\n",
            "Iteration 700, loss = 0.0318\n",
            "Checking accuracy on validation set\n",
            "Got 47258 / 48000 correct (98.45)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We need to wrap `flatten` function in a module in order to stack it\n",
        "# in nn.Sequential\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return flatten(x)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model = nn.Sequential(\n",
        "    ################################################################################\n",
        "    # TODO                                                                         #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        ")\n",
        "\n",
        "# you can use Nesterov momentum in optim.SGD\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_part34(model, optimizer, epochs = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0RoJmFvy9TH"
      },
      "source": [
        "# 문제 13. Part VI. 결과분석\n",
        "\n",
        "* 여러분들이 방금 만든 model은 3계층 MLP 모델입니다. 처음 데이터 분석에서 t-SNE를 활용하여, image를 projection 했던 것 기억하시나요? 이번에는 각 계층의 결과값마다 projection을 적용하여 결과를 비교해봅시다.\n",
        "\n",
        "* 저희의 모델은 다음과 같이 이루어져 있습니다.\n",
        "  1. output dimension이 256인 fully-coonected layer\n",
        "  2. ReLU\n",
        "  3. output dimension이 64인 fully-coonected layer\n",
        "  4. ReLU\n",
        "  5. 10개의 클래스에 대한 점수를 계산하기 위한 fully-coonected layer\n",
        "\n",
        "* 첫번째 코드는 image 자체를 projection한 결과입니다.\n",
        "* 두번째 코드는 model에 1번까지 적용하고 나온 256-dimension 출력값울 projection한 결과입니다.\n",
        "* 세번째 코드는 model에 3번까지 적용하고 나온 64-dimension 출력값을 projection한 결과입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M_ApiXPUDWs7"
      },
      "outputs": [],
      "source": [
        "intermediate_model = nn.Sequential(nn.Flatten())\n",
        "\n",
        "# Prepare the data\n",
        "n_samples = 2000\n",
        "total = 0\n",
        "intermediate_output = []\n",
        "tmp_labels = []\n",
        "for data in test_dataloader:\n",
        "  images, labels = data\n",
        "  intermediate_output.extend(intermediate_model(images).cpu().detach().numpy())\n",
        "  tmp_labels.extend(labels)\n",
        "\n",
        "intermediate_output = np.array(intermediate_output)\n",
        "tmp_labels = np.array(tmp_labels)\n",
        "\n",
        "indices = np.random.choice(len(intermediate_output), n_samples, replace=False)\n",
        "images_subset = intermediate_output[indices]\n",
        "labels_subset = tmp_labels[indices]\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(images_subset)\n",
        "\n",
        "# Plot the t-SNE results\n",
        "plt.figure(figsize=(12, 8))\n",
        "for label in np.unique(labels_subset):\n",
        "    indices = labels_subset == label\n",
        "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=label, alpha=0.5)\n",
        "plt.legend()\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE projection of MNIST dataset (784-dimensional embeddings)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rSGrnr3xDYe_"
      },
      "outputs": [],
      "source": [
        "intermediate_model = nn.Sequential(*list(model.children())[:-4])\n",
        "\n",
        "# Prepare the data\n",
        "n_samples = 2000\n",
        "total = 0\n",
        "intermediate_output = []\n",
        "tmp_labels = []\n",
        "for data in test_dataloader:\n",
        "  images, labels = data\n",
        "  images = images.to(device)\n",
        "  intermediate_output.extend(intermediate_model(images).cpu().detach().numpy())\n",
        "  tmp_labels.extend(labels)\n",
        "\n",
        "intermediate_output = np.array(intermediate_output)\n",
        "tmp_labels = np.array(tmp_labels)\n",
        "\n",
        "indices = np.random.choice(len(intermediate_output), n_samples, replace=False)\n",
        "images_subset = intermediate_output[indices]\n",
        "labels_subset = tmp_labels[indices]\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(images_subset)\n",
        "\n",
        "# Plot the t-SNE results\n",
        "plt.figure(figsize=(12, 8))\n",
        "for label in np.unique(labels_subset):\n",
        "    indices = labels_subset == label\n",
        "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=label, alpha=0.5)\n",
        "plt.legend()\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE projection of MNIST dataset (256-dimensional embeddings)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b0UwsEZrDZrk"
      },
      "outputs": [],
      "source": [
        "intermediate_model = nn.Sequential(*list(model.children())[:-2])\n",
        "\n",
        "# Prepare the data\n",
        "n_samples = 2000\n",
        "total = 0\n",
        "intermediate_output = []\n",
        "tmp_labels = []\n",
        "for data in test_dataloader:\n",
        "  images, labels = data\n",
        "  images = images.to(device)\n",
        "  intermediate_output.extend(intermediate_model(images).cpu().detach().numpy())\n",
        "  tmp_labels.extend(labels)\n",
        "\n",
        "intermediate_output = np.array(intermediate_output)\n",
        "tmp_labels = np.array(tmp_labels)\n",
        "\n",
        "indices = np.random.choice(len(intermediate_output), n_samples, replace=False)\n",
        "images_subset = intermediate_output[indices]\n",
        "labels_subset = tmp_labels[indices]\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(images_subset)\n",
        "\n",
        "# Plot the t-SNE results\n",
        "plt.figure(figsize=(12, 8))\n",
        "for label in np.unique(labels_subset):\n",
        "    indices = labels_subset == label\n",
        "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=label, alpha=0.5)\n",
        "plt.legend()\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE projection of MNIST dataset (64-dimensional embeddings)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8kwaunfDbij"
      },
      "source": [
        "**문제 13**\n",
        "\n",
        "결과를 분석해보세요! : 학습 레이어가 깊어질수록 t-SNE projection 을 했을 때 클래스별 군집화가 더 뚜렷하게 이루어졌다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqeyS5NPrYcL"
      },
      "source": [
        "# 문제 14, 15, 16. Part VII. Optimizer\n",
        "\n",
        "* Optimizer에는 굉장히 많은 종류가 있죠. 저희는 오늘 그 중에서 SGD / Momentum / RMSprop / Adam 이렇게 4개의 optimizer를 비교해 보려고 합니다.\n",
        "\n",
        "* 결과를 시각화한 자료를 보고 마음껏 분석해주세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRAB_E5Q1OnB"
      },
      "source": [
        "---\n",
        "SGD와 Momentum을 비교하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wPsZkn6NYtKG"
      },
      "outputs": [],
      "source": [
        "def poly4_torch(x):\n",
        "    return 0.1 * x**4 - 0.5 * x**3 + 0.2 * x**2 + 2 * x + 1\n",
        "\n",
        "x_init = 4.0\n",
        "x_sgd = torch.tensor([x_init], requires_grad=True)\n",
        "x_momentum = torch.tensor([x_init], requires_grad=True)\n",
        "\n",
        "optimizer_sgd = optim.SGD([x_sgd], lr=0.01)\n",
        "optimizer_momentum = optim.SGD([x_momentum], lr=0.01, momentum=0.9)\n",
        "\n",
        "iterations = 1000\n",
        "\n",
        "x_values_sgd = [x_sgd.item()]\n",
        "x_values_momentum = [x_momentum.item()]\n",
        "\n",
        "loss_values_sgd = [poly4_torch(x_sgd).item()]\n",
        "loss_values_momentum = [poly4_torch(x_momentum).item()]\n",
        "\n",
        "for _ in range(iterations):\n",
        "\n",
        "    optimizer_sgd.zero_grad()\n",
        "    loss_sgd = poly4_torch(x_sgd)\n",
        "    loss_sgd.backward()\n",
        "    optimizer_sgd.step()\n",
        "    x_values_sgd.append(x_sgd.item())\n",
        "    loss_values_sgd.append(loss_sgd.item())\n",
        "\n",
        "    optimizer_momentum.zero_grad()\n",
        "    loss_momentum = poly4_torch(x_momentum)\n",
        "    loss_momentum.backward()\n",
        "    optimizer_momentum.step()\n",
        "    x_values_momentum.append(x_momentum.item())\n",
        "    loss_values_momentum.append(loss_momentum.item())\n",
        "\n",
        "x_range = torch.linspace(-2, 4, 500)\n",
        "y_range = poly4_torch(x_range).detach().numpy()\n",
        "\n",
        "def plot_optimizer(x_values, label, color):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(x_range, y_range, label='poly4_torch function', color='black')\n",
        "    plt.scatter(x_values, [poly4_torch(torch.tensor(x)).item() for x in x_values], label=label, color=color, alpha=0.5)\n",
        "    plt.xlabel('x value')\n",
        "    plt.ylabel('Loss value')\n",
        "    plt.title(f'Loss Function with {label} Optimization Steps')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_optimizer(x_values_sgd, 'SGD', 'green')\n",
        "plot_optimizer(x_values_momentum, 'Momentum', 'red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5R6d6fT1z--"
      },
      "source": [
        "**문제 14**\n",
        "\n",
        "* 다음과 같은 그래프가 그려지는 이유를 설명해주세요 : Momentum 은 SGD 에 물리적인 '관성' 개념을 도입한 것이다. 현재 기울기에 이전에 이동하던 방향과 크기를 반영하는 것이다. SGD 그래프를 보면 local minima 에서 학습이 끝나지만, Momentum 그래프를 보면 local minima 까지 내려가던 힘이 반영되어 local 에 그치지 않고 global minima 까지 학습하는 것을 볼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT0rQSJj1ujO"
      },
      "source": [
        "Momentum과 Adam을 비교하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1n-BTxlUYtgC"
      },
      "outputs": [],
      "source": [
        "def poly2d_torch(x, y):\n",
        "    return 0.15 * (x**4 + y**4) - 0.5 * (x**3 + y**3) + 0.2 * (x**2 + y**2) + 2 * (x + y) + 1\n",
        "\n",
        "x_init = torch.tensor([-2.0], requires_grad=True)\n",
        "y_init = torch.tensor([4.0], requires_grad=True)\n",
        "\n",
        "x_adam = x_init.clone().detach().requires_grad_(True)\n",
        "y_adam = y_init.clone().detach().requires_grad_(True)\n",
        "\n",
        "x_momentum = x_init.clone().detach().requires_grad_(True)\n",
        "y_momentum = y_init.clone().detach().requires_grad_(True)\n",
        "\n",
        "optimizer_adam = optim.Adam([x_adam, y_adam], lr=0.01)\n",
        "optimizer_momentum = optim.SGD([x_momentum, y_momentum], lr=0.01, momentum=0.9)\n",
        "\n",
        "iterations = 10000\n",
        "\n",
        "x_values_adam = [x_adam.item()]\n",
        "y_values_adam = [y_adam.item()]\n",
        "loss_values_adam = [poly2d_torch(x_adam, y_adam).item()]\n",
        "\n",
        "x_values_momentum = [x_momentum.item()]\n",
        "y_values_momentum = [y_momentum.item()]\n",
        "loss_values_momentum = [poly2d_torch(x_momentum, y_momentum).item()]\n",
        "\n",
        "for _ in tqdm(range(iterations)):\n",
        "    optimizer_adam.zero_grad()\n",
        "    loss_adam = poly2d_torch(x_adam, y_adam)\n",
        "    loss_adam.backward()\n",
        "    optimizer_adam.step()\n",
        "    x_values_adam.append(x_adam.item())\n",
        "    y_values_adam.append(y_adam.item())\n",
        "    loss_values_adam.append(loss_adam.item())\n",
        "\n",
        "    optimizer_momentum.zero_grad()\n",
        "    loss_momentum = poly2d_torch(x_momentum, y_momentum)\n",
        "    loss_momentum.backward()\n",
        "    optimizer_momentum.step()\n",
        "    x_values_momentum.append(x_momentum.item())\n",
        "    y_values_momentum.append(y_momentum.item())\n",
        "    loss_values_momentum.append(loss_momentum.item())\n",
        "\n",
        "x_range = torch.linspace(-2, 4, 100)\n",
        "y_range = torch.linspace(-2, 4, 100)\n",
        "X, Y = torch.meshgrid(x_range, y_range)\n",
        "Z = poly2d_torch(X, Y).detach().numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z, cmap='viridis', alpha=0.6)\n",
        "\n",
        "ax.scatter(x_values_adam, y_values_adam, loss_values_adam, label='Adam', color='orange')\n",
        "ax.scatter(x_values_momentum, y_values_momentum, loss_values_momentum, label='Momentum', color='red')\n",
        "\n",
        "ax.set_xlabel('x value')\n",
        "ax.set_ylabel('y value')\n",
        "ax.set_zlabel('Loss value')\n",
        "ax.set_title('Loss Function with Adam and Momentum Optimization Steps')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY3-7vN317LJ"
      },
      "source": [
        "**문제 15**\n",
        "\n",
        "* 다음과 같은 그래프가 그려지는 이유를 설명해주세요 (인터넷에 검색하셔도 좋습니다!): Adam 은 Momentum 과 달리, loss 함수의 경사에 따라 방향과 학습률(learning rate) 를 스스로 조절하는 특징이 있다. 따라서 Momentum 은 관성 때문에 목적지에 수렴하는 데 오래 걸리는 반면, Adam 은 훨씬 직선적이고 효율적인 경로를 택해 빠르게 최저점을 찾는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zHmz5mYffFl2"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_part34(model, criterion, optimizer, epochs=10):\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    train_loss = []\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(train_dataloader):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = criterion(scores, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print(f'Epoch {e+1}, Iteration {t}, loss = {loss.item():.4f}')\n",
        "                train_loss.append(loss.item())\n",
        "    return train_loss\n",
        "\n",
        "# Initialize models and optimizers\n",
        "optimizers = {\n",
        "    'RMSProp': optim.RMSprop,\n",
        "    'Adam': optim.Adam,\n",
        "    'SGD': optim.SGD,\n",
        "    'Momentum': lambda params, lr: optim.SGD(params, lr=lr, momentum=0.9)\n",
        "}\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train models and collect loss\n",
        "losses = {}\n",
        "for opt_name, opt_class in optimizers.items():\n",
        "    print(f\"Training with {opt_name} optimizer\")\n",
        "    model = SimpleModel()\n",
        "    optimizer = opt_class(model.parameters(), lr=learning_rate)\n",
        "    losses[opt_name] = train_part34(model, criterion, optimizer)\n",
        "\n",
        "# Plotting the losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for opt_name, loss in losses.items():\n",
        "    plt.plot(loss, label=opt_name)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss with Different Optimizers')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3skNMwW2Brh"
      },
      "source": [
        "**문제 16**\n",
        "\n",
        "* 결과를 마음껏 분석해주세요: Adam 과 RMSProp 은 그래프 시작과 동시에 손실 값이 수직에 가깝게 떨어진다. 두 최적화 알고리즘은 모두 학습률을 자동으로 조절하는 능력이 뛰어나기 때문에 초기 수렴 속도가 압도적으로 빠르다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH8oqtnTescU"
      },
      "source": [
        "# 문제 17, 18.Part VIII 자신만의 모델 만들기!\n",
        "\n",
        "* 이제 위에서 배운 내용을 바탕으로 자유롭게 모델을 만들어주세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fdwGwz301_"
      },
      "source": [
        "**문제 17**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU8o5MfEexI3",
        "collapsed": true,
        "outputId": "2a4678f7-a953-40b9-a7d9-1fea949b5be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 2.2775\n",
            "Checking accuracy on validation set\n",
            "Got 4871 / 48000 correct (10.15)\n",
            "\n",
            "Iteration 100, loss = 0.3254\n",
            "Checking accuracy on validation set\n",
            "Got 45030 / 48000 correct (93.81)\n",
            "\n",
            "Iteration 200, loss = 0.1383\n",
            "Checking accuracy on validation set\n",
            "Got 46226 / 48000 correct (96.30)\n",
            "\n",
            "Iteration 300, loss = 0.1581\n",
            "Checking accuracy on validation set\n",
            "Got 46558 / 48000 correct (97.00)\n",
            "\n",
            "Iteration 400, loss = 0.1048\n",
            "Checking accuracy on validation set\n",
            "Got 46973 / 48000 correct (97.86)\n",
            "\n",
            "Iteration 500, loss = 0.1529\n",
            "Checking accuracy on validation set\n",
            "Got 47088 / 48000 correct (98.10)\n",
            "\n",
            "Iteration 600, loss = 0.1803\n",
            "Checking accuracy on validation set\n",
            "Got 47168 / 48000 correct (98.27)\n",
            "\n",
            "Iteration 700, loss = 0.1317\n",
            "Checking accuracy on validation set\n",
            "Got 47249 / 48000 correct (98.44)\n",
            "\n",
            "Iteration 0, loss = 0.0312\n",
            "Checking accuracy on validation set\n",
            "Got 47299 / 48000 correct (98.54)\n",
            "\n",
            "Iteration 100, loss = 0.1080\n",
            "Checking accuracy on validation set\n",
            "Got 47236 / 48000 correct (98.41)\n",
            "\n",
            "Iteration 200, loss = 0.0722\n",
            "Checking accuracy on validation set\n",
            "Got 47367 / 48000 correct (98.68)\n",
            "\n",
            "Iteration 300, loss = 0.0747\n",
            "Checking accuracy on validation set\n",
            "Got 47379 / 48000 correct (98.71)\n",
            "\n",
            "Iteration 400, loss = 0.0171\n",
            "Checking accuracy on validation set\n",
            "Got 47382 / 48000 correct (98.71)\n",
            "\n",
            "Iteration 500, loss = 0.0458\n",
            "Checking accuracy on validation set\n",
            "Got 47528 / 48000 correct (99.02)\n",
            "\n",
            "Iteration 600, loss = 0.0690\n",
            "Checking accuracy on validation set\n",
            "Got 47460 / 48000 correct (98.88)\n",
            "\n",
            "Iteration 700, loss = 0.0084\n",
            "Checking accuracy on validation set\n",
            "Got 47537 / 48000 correct (99.04)\n",
            "\n",
            "Iteration 0, loss = 0.0281\n",
            "Checking accuracy on validation set\n",
            "Got 47581 / 48000 correct (99.13)\n",
            "\n",
            "Iteration 100, loss = 0.0310\n",
            "Checking accuracy on validation set\n",
            "Got 47602 / 48000 correct (99.17)\n",
            "\n",
            "Iteration 200, loss = 0.1008\n",
            "Checking accuracy on validation set\n",
            "Got 47603 / 48000 correct (99.17)\n",
            "\n",
            "Iteration 300, loss = 0.0078\n",
            "Checking accuracy on validation set\n",
            "Got 47521 / 48000 correct (99.00)\n",
            "\n",
            "Iteration 400, loss = 0.0211\n",
            "Checking accuracy on validation set\n",
            "Got 47613 / 48000 correct (99.19)\n",
            "\n",
            "Iteration 500, loss = 0.0630\n",
            "Checking accuracy on validation set\n",
            "Got 47644 / 48000 correct (99.26)\n",
            "\n",
            "Iteration 600, loss = 0.0077\n",
            "Checking accuracy on validation set\n",
            "Got 47625 / 48000 correct (99.22)\n",
            "\n",
            "Iteration 700, loss = 0.0431\n",
            "Checking accuracy on validation set\n",
            "Got 47714 / 48000 correct (99.40)\n",
            "\n",
            "Iteration 0, loss = 0.0062\n",
            "Checking accuracy on validation set\n",
            "Got 47692 / 48000 correct (99.36)\n",
            "\n",
            "Iteration 100, loss = 0.1175\n",
            "Checking accuracy on validation set\n",
            "Got 47639 / 48000 correct (99.25)\n",
            "\n",
            "Iteration 200, loss = 0.0070\n",
            "Checking accuracy on validation set\n",
            "Got 47755 / 48000 correct (99.49)\n",
            "\n",
            "Iteration 300, loss = 0.0172\n",
            "Checking accuracy on validation set\n",
            "Got 47774 / 48000 correct (99.53)\n",
            "\n",
            "Iteration 400, loss = 0.0286\n",
            "Checking accuracy on validation set\n",
            "Got 47728 / 48000 correct (99.43)\n",
            "\n",
            "Iteration 500, loss = 0.0339\n",
            "Checking accuracy on validation set\n",
            "Got 47718 / 48000 correct (99.41)\n",
            "\n",
            "Iteration 600, loss = 0.0140\n",
            "Checking accuracy on validation set\n",
            "Got 47755 / 48000 correct (99.49)\n",
            "\n",
            "Iteration 700, loss = 0.0112\n",
            "Checking accuracy on validation set\n",
            "Got 47795 / 48000 correct (99.57)\n",
            "\n",
            "Iteration 0, loss = 0.0118\n",
            "Checking accuracy on validation set\n",
            "Got 47738 / 48000 correct (99.45)\n",
            "\n",
            "Iteration 100, loss = 0.0650\n",
            "Checking accuracy on validation set\n",
            "Got 47607 / 48000 correct (99.18)\n",
            "\n",
            "Iteration 200, loss = 0.0289\n",
            "Checking accuracy on validation set\n",
            "Got 47801 / 48000 correct (99.59)\n",
            "\n",
            "Iteration 300, loss = 0.0109\n",
            "Checking accuracy on validation set\n",
            "Got 47794 / 48000 correct (99.57)\n",
            "\n",
            "Iteration 400, loss = 0.0332\n",
            "Checking accuracy on validation set\n",
            "Got 47810 / 48000 correct (99.60)\n",
            "\n",
            "Iteration 500, loss = 0.0079\n",
            "Checking accuracy on validation set\n",
            "Got 47814 / 48000 correct (99.61)\n",
            "\n",
            "Iteration 600, loss = 0.0017\n",
            "Checking accuracy on validation set\n",
            "Got 47784 / 48000 correct (99.55)\n",
            "\n",
            "Iteration 700, loss = 0.0074\n",
            "Checking accuracy on validation set\n",
            "Got 47593 / 48000 correct (99.15)\n",
            "\n",
            "Iteration 0, loss = 0.0070\n",
            "Checking accuracy on validation set\n",
            "Got 47647 / 48000 correct (99.26)\n",
            "\n",
            "Iteration 100, loss = 0.0012\n",
            "Checking accuracy on validation set\n",
            "Got 47875 / 48000 correct (99.74)\n",
            "\n",
            "Iteration 200, loss = 0.0320\n",
            "Checking accuracy on validation set\n",
            "Got 47846 / 48000 correct (99.68)\n",
            "\n",
            "Iteration 300, loss = 0.0138\n",
            "Checking accuracy on validation set\n",
            "Got 47843 / 48000 correct (99.67)\n",
            "\n",
            "Iteration 400, loss = 0.1296\n",
            "Checking accuracy on validation set\n",
            "Got 47819 / 48000 correct (99.62)\n",
            "\n",
            "Iteration 500, loss = 0.0124\n",
            "Checking accuracy on validation set\n",
            "Got 47884 / 48000 correct (99.76)\n",
            "\n",
            "Iteration 600, loss = 0.0036\n",
            "Checking accuracy on validation set\n",
            "Got 47895 / 48000 correct (99.78)\n",
            "\n",
            "Iteration 700, loss = 0.0033\n",
            "Checking accuracy on validation set\n",
            "Got 47888 / 48000 correct (99.77)\n",
            "\n",
            "Iteration 0, loss = 0.0206\n",
            "Checking accuracy on validation set\n",
            "Got 47903 / 48000 correct (99.80)\n",
            "\n",
            "Iteration 100, loss = 0.0092\n",
            "Checking accuracy on validation set\n",
            "Got 47905 / 48000 correct (99.80)\n",
            "\n",
            "Iteration 200, loss = 0.0236\n",
            "Checking accuracy on validation set\n",
            "Got 47872 / 48000 correct (99.73)\n",
            "\n",
            "Iteration 300, loss = 0.0092\n",
            "Checking accuracy on validation set\n",
            "Got 47841 / 48000 correct (99.67)\n",
            "\n",
            "Iteration 400, loss = 0.0013\n",
            "Checking accuracy on validation set\n",
            "Got 47925 / 48000 correct (99.84)\n",
            "\n",
            "Iteration 500, loss = 0.0076\n",
            "Checking accuracy on validation set\n",
            "Got 47918 / 48000 correct (99.83)\n",
            "\n",
            "Iteration 600, loss = 0.0205\n",
            "Checking accuracy on validation set\n",
            "Got 47915 / 48000 correct (99.82)\n",
            "\n",
            "Iteration 700, loss = 0.0441\n",
            "Checking accuracy on validation set\n",
            "Got 47904 / 48000 correct (99.80)\n",
            "\n",
            "Iteration 0, loss = 0.0076\n",
            "Checking accuracy on validation set\n",
            "Got 47924 / 48000 correct (99.84)\n",
            "\n",
            "Iteration 100, loss = 0.0019\n",
            "Checking accuracy on validation set\n",
            "Got 47936 / 48000 correct (99.87)\n",
            "\n",
            "Iteration 200, loss = 0.0113\n",
            "Checking accuracy on validation set\n",
            "Got 47908 / 48000 correct (99.81)\n",
            "\n",
            "Iteration 300, loss = 0.0084\n",
            "Checking accuracy on validation set\n",
            "Got 47942 / 48000 correct (99.88)\n",
            "\n",
            "Iteration 400, loss = 0.0068\n",
            "Checking accuracy on validation set\n",
            "Got 47931 / 48000 correct (99.86)\n",
            "\n",
            "Iteration 500, loss = 0.0054\n",
            "Checking accuracy on validation set\n",
            "Got 47900 / 48000 correct (99.79)\n",
            "\n",
            "Iteration 600, loss = 0.0027\n",
            "Checking accuracy on validation set\n",
            "Got 47930 / 48000 correct (99.85)\n",
            "\n",
            "Iteration 700, loss = 0.0181\n",
            "Checking accuracy on validation set\n",
            "Got 47910 / 48000 correct (99.81)\n",
            "\n",
            "Iteration 0, loss = 0.0076\n",
            "Checking accuracy on validation set\n",
            "Got 47959 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 100, loss = 0.0016\n",
            "Checking accuracy on validation set\n",
            "Got 47911 / 48000 correct (99.81)\n",
            "\n",
            "Iteration 200, loss = 0.0020\n",
            "Checking accuracy on validation set\n",
            "Got 47955 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 300, loss = 0.0022\n",
            "Checking accuracy on validation set\n",
            "Got 47923 / 48000 correct (99.84)\n",
            "\n",
            "Iteration 400, loss = 0.0215\n",
            "Checking accuracy on validation set\n",
            "Got 47953 / 48000 correct (99.90)\n",
            "\n",
            "Iteration 500, loss = 0.0161\n",
            "Checking accuracy on validation set\n",
            "Got 47941 / 48000 correct (99.88)\n",
            "\n",
            "Iteration 600, loss = 0.0027\n",
            "Checking accuracy on validation set\n",
            "Got 47950 / 48000 correct (99.90)\n",
            "\n",
            "Iteration 700, loss = 0.0167\n",
            "Checking accuracy on validation set\n",
            "Got 47966 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 0, loss = 0.0017\n",
            "Checking accuracy on validation set\n",
            "Got 47959 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 100, loss = 0.0013\n",
            "Checking accuracy on validation set\n",
            "Got 47942 / 48000 correct (99.88)\n",
            "\n",
            "Iteration 200, loss = 0.0011\n",
            "Checking accuracy on validation set\n",
            "Got 47913 / 48000 correct (99.82)\n",
            "\n",
            "Iteration 300, loss = 0.0033\n",
            "Checking accuracy on validation set\n",
            "Got 47977 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 400, loss = 0.0009\n",
            "Checking accuracy on validation set\n",
            "Got 47974 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 500, loss = 0.0117\n",
            "Checking accuracy on validation set\n",
            "Got 47941 / 48000 correct (99.88)\n",
            "\n",
            "Iteration 600, loss = 0.0035\n",
            "Checking accuracy on validation set\n",
            "Got 47955 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 700, loss = 0.0011\n",
            "Checking accuracy on validation set\n",
            "Got 47955 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 0, loss = 0.0021\n",
            "Checking accuracy on validation set\n",
            "Got 47936 / 48000 correct (99.87)\n",
            "\n",
            "Iteration 100, loss = 0.0011\n",
            "Checking accuracy on validation set\n",
            "Got 47889 / 48000 correct (99.77)\n",
            "\n",
            "Iteration 200, loss = 0.0035\n",
            "Checking accuracy on validation set\n",
            "Got 47963 / 48000 correct (99.92)\n",
            "\n",
            "Iteration 300, loss = 0.0039\n",
            "Checking accuracy on validation set\n",
            "Got 47966 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 400, loss = 0.0015\n",
            "Checking accuracy on validation set\n",
            "Got 47959 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 500, loss = 0.0006\n",
            "Checking accuracy on validation set\n",
            "Got 47978 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 600, loss = 0.0012\n",
            "Checking accuracy on validation set\n",
            "Got 47978 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 700, loss = 0.1198\n",
            "Checking accuracy on validation set\n",
            "Got 47917 / 48000 correct (99.83)\n",
            "\n",
            "Iteration 0, loss = 0.0009\n",
            "Checking accuracy on validation set\n",
            "Got 47960 / 48000 correct (99.92)\n",
            "\n",
            "Iteration 100, loss = 0.0022\n",
            "Checking accuracy on validation set\n",
            "Got 47987 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 200, loss = 0.0138\n",
            "Checking accuracy on validation set\n",
            "Got 47978 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 300, loss = 0.0188\n",
            "Checking accuracy on validation set\n",
            "Got 47982 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 400, loss = 0.0043\n",
            "Checking accuracy on validation set\n",
            "Got 47960 / 48000 correct (99.92)\n",
            "\n",
            "Iteration 500, loss = 0.0056\n",
            "Checking accuracy on validation set\n",
            "Got 47977 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 600, loss = 0.0046\n",
            "Checking accuracy on validation set\n",
            "Got 47981 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 700, loss = 0.0105\n",
            "Checking accuracy on validation set\n",
            "Got 47948 / 48000 correct (99.89)\n",
            "\n",
            "Iteration 0, loss = 0.0020\n",
            "Checking accuracy on validation set\n",
            "Got 47962 / 48000 correct (99.92)\n",
            "\n",
            "Iteration 100, loss = 0.0009\n",
            "Checking accuracy on validation set\n",
            "Got 47982 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 200, loss = 0.0015\n",
            "Checking accuracy on validation set\n",
            "Got 47983 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 300, loss = 0.0074\n",
            "Checking accuracy on validation set\n",
            "Got 47989 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 400, loss = 0.0003\n",
            "Checking accuracy on validation set\n",
            "Got 47983 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 500, loss = 0.0085\n",
            "Checking accuracy on validation set\n",
            "Got 47966 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 600, loss = 0.0102\n",
            "Checking accuracy on validation set\n",
            "Got 47986 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 700, loss = 0.0313\n",
            "Checking accuracy on validation set\n",
            "Got 47976 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 0, loss = 0.0003\n",
            "Checking accuracy on validation set\n",
            "Got 47991 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 100, loss = 0.0005\n",
            "Checking accuracy on validation set\n",
            "Got 47965 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 200, loss = 0.0251\n",
            "Checking accuracy on validation set\n",
            "Got 47978 / 48000 correct (99.95)\n",
            "\n",
            "Iteration 300, loss = 0.0007\n",
            "Checking accuracy on validation set\n",
            "Got 47859 / 48000 correct (99.71)\n",
            "\n",
            "Iteration 400, loss = 0.0023\n",
            "Checking accuracy on validation set\n",
            "Got 47973 / 48000 correct (99.94)\n",
            "\n",
            "Iteration 500, loss = 0.0022\n",
            "Checking accuracy on validation set\n",
            "Got 47987 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 600, loss = 0.0003\n",
            "Checking accuracy on validation set\n",
            "Got 47958 / 48000 correct (99.91)\n",
            "\n",
            "Iteration 700, loss = 0.0002\n",
            "Checking accuracy on validation set\n",
            "Got 47984 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 0, loss = 0.0013\n",
            "Checking accuracy on validation set\n",
            "Got 47965 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 100, loss = 0.0017\n",
            "Checking accuracy on validation set\n",
            "Got 47970 / 48000 correct (99.94)\n",
            "\n",
            "Iteration 200, loss = 0.0025\n",
            "Checking accuracy on validation set\n",
            "Got 47995 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 300, loss = 0.0070\n",
            "Checking accuracy on validation set\n",
            "Got 47990 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 400, loss = 0.0018\n",
            "Checking accuracy on validation set\n",
            "Got 47991 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 500, loss = 0.0044\n",
            "Checking accuracy on validation set\n",
            "Got 47983 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 600, loss = 0.0009\n",
            "Checking accuracy on validation set\n",
            "Got 47989 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 700, loss = 0.0040\n",
            "Checking accuracy on validation set\n",
            "Got 47989 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 0, loss = 0.0010\n",
            "Checking accuracy on validation set\n",
            "Got 47996 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 100, loss = 0.0014\n",
            "Checking accuracy on validation set\n",
            "Got 47981 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 200, loss = 0.0066\n",
            "Checking accuracy on validation set\n",
            "Got 47991 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 300, loss = 0.0000\n",
            "Checking accuracy on validation set\n",
            "Got 47973 / 48000 correct (99.94)\n",
            "\n",
            "Iteration 400, loss = 0.0012\n",
            "Checking accuracy on validation set\n",
            "Got 47999 / 48000 correct (100.00)\n",
            "\n",
            "Iteration 500, loss = 0.0008\n",
            "Checking accuracy on validation set\n",
            "Got 47994 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 600, loss = 0.0045\n",
            "Checking accuracy on validation set\n",
            "Got 47991 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 700, loss = 0.0246\n",
            "Checking accuracy on validation set\n",
            "Got 47995 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 0, loss = 0.0010\n",
            "Checking accuracy on validation set\n",
            "Got 47892 / 48000 correct (99.78)\n",
            "\n",
            "Iteration 100, loss = 0.0002\n",
            "Checking accuracy on validation set\n",
            "Got 47999 / 48000 correct (100.00)\n",
            "\n",
            "Iteration 200, loss = 0.0078\n",
            "Checking accuracy on validation set\n",
            "Got 47946 / 48000 correct (99.89)\n",
            "\n",
            "Iteration 300, loss = 0.0001\n",
            "Checking accuracy on validation set\n",
            "Got 47986 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 400, loss = 0.0011\n",
            "Checking accuracy on validation set\n",
            "Got 47965 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 500, loss = 0.0019\n",
            "Checking accuracy on validation set\n",
            "Got 47990 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 600, loss = 0.0004\n",
            "Checking accuracy on validation set\n",
            "Got 47986 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 700, loss = 0.0006\n",
            "Checking accuracy on validation set\n",
            "Got 47991 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 0, loss = 0.0002\n",
            "Checking accuracy on validation set\n",
            "Got 47984 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 100, loss = 0.0197\n",
            "Checking accuracy on validation set\n",
            "Got 47988 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 200, loss = 0.0421\n",
            "Checking accuracy on validation set\n",
            "Got 47988 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 300, loss = 0.0032\n",
            "Checking accuracy on validation set\n",
            "Got 47992 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 400, loss = 0.0010\n",
            "Checking accuracy on validation set\n",
            "Got 47989 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 500, loss = 0.0053\n",
            "Checking accuracy on validation set\n",
            "Got 47993 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 600, loss = 0.0020\n",
            "Checking accuracy on validation set\n",
            "Got 47984 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 700, loss = 0.0007\n",
            "Checking accuracy on validation set\n",
            "Got 47962 / 48000 correct (99.92)\n",
            "\n",
            "Iteration 0, loss = 0.0046\n",
            "Checking accuracy on validation set\n",
            "Got 47967 / 48000 correct (99.93)\n",
            "\n",
            "Iteration 100, loss = 0.0006\n",
            "Checking accuracy on validation set\n",
            "Got 47988 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 200, loss = 0.0314\n",
            "Checking accuracy on validation set\n",
            "Got 47988 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 300, loss = 0.0007\n",
            "Checking accuracy on validation set\n",
            "Got 47986 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 400, loss = 0.0002\n",
            "Checking accuracy on validation set\n",
            "Got 47990 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 500, loss = 0.0012\n",
            "Checking accuracy on validation set\n",
            "Got 47986 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 600, loss = 0.0014\n",
            "Checking accuracy on validation set\n",
            "Got 47998 / 48000 correct (100.00)\n",
            "\n",
            "Iteration 700, loss = 0.0000\n",
            "Checking accuracy on validation set\n",
            "Got 47990 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 0, loss = 0.0000\n",
            "Checking accuracy on validation set\n",
            "Got 47993 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 100, loss = 0.0007\n",
            "Checking accuracy on validation set\n",
            "Got 47935 / 48000 correct (99.86)\n",
            "\n",
            "Iteration 200, loss = 0.0077\n",
            "Checking accuracy on validation set\n",
            "Got 47989 / 48000 correct (99.98)\n",
            "\n",
            "Iteration 300, loss = 0.0002\n",
            "Checking accuracy on validation set\n",
            "Got 47979 / 48000 correct (99.96)\n",
            "\n",
            "Iteration 400, loss = 0.0006\n",
            "Checking accuracy on validation set\n",
            "Got 47970 / 48000 correct (99.94)\n",
            "\n",
            "Iteration 500, loss = 0.0002\n",
            "Checking accuracy on validation set\n",
            "Got 47985 / 48000 correct (99.97)\n",
            "\n",
            "Iteration 600, loss = 0.0001\n",
            "Checking accuracy on validation set\n",
            "Got 47993 / 48000 correct (99.99)\n",
            "\n",
            "Iteration 700, loss = 0.0075\n",
            "Checking accuracy on validation set\n",
            "Got 47988 / 48000 correct (99.98)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 차원 변경\n",
        "class PermuteLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # [Batch, 28, 1, 28] -> [Batch, 1, 28, 28]\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model = nn.Sequential(\n",
        "  PermuteLayer(),\n",
        "\n",
        "  nn.Conv2d(1, 32, kernel_size=3, padding=1), # 28\n",
        "  nn.BatchNorm2d(32),\n",
        "  nn.ReLU(),\n",
        "  nn.MaxPool2d(2,2), # 14\n",
        "\n",
        "  nn.Conv2d(32, 64, kernel_size=3, padding=1), # 14\n",
        "  nn.BatchNorm2d(64),\n",
        "  nn.ReLU(),\n",
        "  nn.MaxPool2d(2,2), # 7\n",
        "\n",
        "  nn.Conv2d(64, 128, kernel_size=3, padding=1), # 7\n",
        "  nn.BatchNorm2d(128),\n",
        "  nn.ReLU(),\n",
        "\n",
        "  nn.Flatten(), # 7 x 7 x 128\n",
        "  nn.Linear(128 * 7 * 7, 256),\n",
        "  nn.ReLU(),\n",
        "  nn.Dropout(0.5), # 과적합 방지\n",
        "  nn.Linear(256, 10),\n",
        "  nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_part34(model, optimizer, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0pQLgSO3cLU"
      },
      "source": [
        "* 밑의 코드에서 높은 Score를 얻으신 분이 1등입니다. 위에는 아무상관없습니다~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKzzT4qG2ROJ",
        "outputId": "4eab5c97-938f-4c67-a68a-b4bdafab4c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on test set\n",
            "Got 47997 / 48000 correct (99.99)\n"
          ]
        }
      ],
      "source": [
        "check_accuracy_part34(test_dataloader, model, istrain=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgnDIpAU3ntm"
      },
      "source": [
        "**문제 18**\n",
        "* 자신의 모델에 대해 설명해주세요~\n",
        "\n",
        "딥인딥에서 이미지 분류에는 CNN 을 많이 사용한다는 점을 배웠기 때문에 처음에는 딥인딥 실습 코드를 보면서 CNN 을 만들어봤는데 정확도가 99.26% 정도로 나왔다. 더 성능을 높이기 위해 mnist sota 모델을 찾다보니 mcdnn 이라는 것을 알게 되었다. mcdnn 은 이미지를 20x20, 10x10 등으로 직접 리사이징해서 10개의 모델에 각각 넣고, 앙상블하는 모델이었다. 하지만, 10개의 모델을 돌릴 자원은 부족하다고 생각해서 하나의 모델에서 층과 채널 수를 늘리는 방식으로 기존 CNN 을 수정하였다. 대신, 아래 세 가지 방식을 추가로 이용했다.\n",
        "- BatchNorm: 학습 속도가 비약적으로 빨라지고, 초기값 설정에 구애받지 않음.\n",
        "- Dropout: MCDNN 처럼 앙상블(여러 모델 합치기)을 못 하는 대신, 뉴런을 껐다 켜며 모델이 강인해짐.\n",
        "- LogSoftmax: 확률 분포의 편차를 조정하여 분류 성능을 최적화함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeNWdsPu4KfV"
      },
      "source": [
        "## **수고 하셨습니다~~**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vtsHWi7KpR1x",
        "viZC941XecG6",
        "m3YssC4pqNA0",
        "5Na9WUj8rMO6",
        "KLO6iHpBpR15",
        "UEL8vbVfu7rJ",
        "tsVplnGJuhDY",
        "yTVAtckopR14",
        "E0RoJmFvy9TH"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3109",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}